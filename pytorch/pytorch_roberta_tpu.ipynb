{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_roberta_tpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMI24+jhOKR0CU9x/uUXtx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/branjbar/nlp-bert-family/blob/master/pytorch/pytorch_roberta_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN-jcaOwcSls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CHANGES:\n",
        "## lr increase\n",
        "## TRAIN_BATCH_SIZE\n",
        "## VALID_BATCH_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt_o8YchvJ98",
        "colab_type": "code",
        "outputId": "21039cc3-deb0-4543-fcd0-7f4dc93ac8b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To avoid notebook get disconnected\n",
        "\"\"\"\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button\").click() \n",
        "}setInterval(ClickConnect,60000)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "caffeinate\n",
        "\"\"\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncaffeinate\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teCsiKGxyoPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODOS:\n",
        "## add tic and toc to calculate the total time!\n",
        "## check the pretrained model\n",
        "## remove neutral from the filtering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgnUUTnjxCOb",
        "colab_type": "code",
        "outputId": "f2742412-9098-4b29-beb4-63145d82ba90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvblIFXV_iJe",
        "colab_type": "code",
        "outputId": "061ce966-b903-45bd-f92b-ce23579fa942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "# Installs PyTorch, PyTorch/XLA, and Torchvision\n",
        "# Copy this cell into your own notebooks to use PyTorch on Cloud TPUs \n",
        "# Warning: this may take a couple minutes to run\n",
        "\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "VERSION = \"20200325\"  # @ param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  3727  100  3727    0     0  17096      0 --:--:-- --:--:-- --:--:-- 17096\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200325 ...\n",
            "Uninstalling torch-1.5.0+cu101:\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "  Successfully uninstalled torch-1.5.0+cu101\n",
            "Uninstalling torchvision-0.6.0+cu101:\n",
            "  Successfully uninstalled torchvision-0.6.0+cu101\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 83.4 MiB/ 83.4 MiB]                                                \n",
            "Operation completed over 1 objects/83.4 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][114.5 MiB/114.5 MiB]                                                \n",
            "Operation completed over 1 objects/114.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (1.18.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (0.16.0)\n",
            "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+d6149a7\n",
            "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "Successfully installed torch-xla-1.6+e788e5b\n",
            "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200325) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  libomp5\n",
            "0 upgraded, 1 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 234 kB of archives.\n",
            "After this operation, 774 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\n",
            "Fetched 234 kB in 1s (329 kB/s)\n",
            "Selecting previously unselected package libomp5:amd64.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\n",
            "Unpacking libomp5:amd64 (5.0.1-1) ...\n",
            "Setting up libomp5:amd64 (5.0.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WShSfuy67F0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM-eZntGvP6E",
        "colab_type": "code",
        "outputId": "2a24feea-35a5-4d60-c29d-99c57fdbd6d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "!pip install transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "import tokenizers\n",
        "import string\n",
        "import transformers\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from sklearn import model_selection\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 18.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/50/93509f906a40bffd7d175f97fd75ea328ad9bd91f48f59c4bd084c94a25e/sacremoses-0.0.41.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 21.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 29.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.41-cp36-none-any.whl size=893334 sha256=5f2773a8984fdbef96fc9abb20a772d35f4d249945ac7046b9fb5e81c79e751e\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/5a/d4/b020a81249de7dc63758a34222feaa668dbe8ebfe9170cc9b1\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.41 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqU8gplkvcHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KAGGLE_INPUT_PATH = \"../input/\"\n",
        "COLAB_INPUT_PATH = \"/content/drive/My Drive/input/\"\n",
        "INPUT_PATH = COLAB_INPUT_PATH\n",
        "\n",
        "DATA_PATH = INPUT_PATH + \"tweet-sentiment-extraction/\"\n",
        "CHECKPOINT_PATH = INPUT_PATH + \"model_checkpoint/roberta/\"\n",
        "CHECKPOINT_PATH_TORCH = INPUT_PATH + \"model_checkpoint/roberta/torch/\"\n",
        "CHECKPOINT_PATH_HP_TUNING = INPUT_PATH + \"model_checkpoint/roberta/hp-tuning/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3lnlEuvEkoJ",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeAY0ubtUk3N",
        "colab_type": "code",
        "outputId": "d68aff6e-63e2-40e6-d4f7-654d1b7db61e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "class Config():\n",
        "    def __init__(self, config_path=None):\n",
        "\n",
        "        if config_path:\n",
        "            with open(config_path,'r') as json_file:\n",
        "                self.__dict__ = json.load(json_file)\n",
        "        else:\n",
        "            \n",
        "            self.version='tpu-v3'\n",
        "\n",
        "            # preprocessing\n",
        "            self.max_len = 192\n",
        "            self.tweet_size_to_ignore = 1\n",
        "\n",
        "            # learning\n",
        "            self.model_path = \"models/roberta-base\"\n",
        "            self.model_arch = \"abhishek\"\n",
        "            self.drop_out_1 = .1\n",
        "            self.lr = 3e-5\n",
        "            self.adamw_weight_decay = 0\n",
        "            self.use_scheduler = True\n",
        "            self.scheduler_weight_decay_1=0.00001\n",
        "            self.scheduler_weight_decay_2=0\n",
        "            self.label_smoothing=False #https://stackoverflow.com/questions/55681502/label-smoothing-in-pytorch\n",
        "            # running\n",
        "            self.train_batch_size = 8\n",
        "            self.valid_batch_size = 16\n",
        "            self.num_epochs = 3\n",
        "            self.num_folds = 5\n",
        "\n",
        "            # dev\n",
        "            self.num_folds_to_train = 5\n",
        "            self.dev = True\n",
        "        \n",
        "        print(str(self.__dict__))\n",
        "\n",
        "    def get(self, var):\n",
        "        return getattr(self, var, None)\n",
        "\n",
        "\n",
        "config_file_name = \"config-json-tpu-v28.txt\"\n",
        "config_file_path = os.path.join(CHECKPOINT_PATH_HP_TUNING,config_file_name)\n",
        "if os.path.exists(config_file_path):\n",
        "    config = Config(config_file_path)\n",
        "else:\n",
        "    print(\"config file not found! Default configuration will be used\")\n",
        "    config = Config()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'version': 'tpu-v28', 'max_len': 192, 'tweet_size_to_ignore': 1, 'model_path': 'models/roberta-twitter3m', 'model_arch': 'abhishek', 'drop_out_1': 0.3, 'lr': 5e-05, 'adam_type': 'type_1', 'use_scheduler': True, 'scheduler_weight_decay_1': 0.01, 'scheduler_weight_decay_2': 0, 'smoothing_coeff': 0.25, 'gradient_clipping': True, 'max_grad_norm': 0.1, 'train_batch_size': 8, 'valid_batch_size': 16, 'num_epochs': 3, 'num_folds': 5, 'num_folds_to_train': 1, 'dev': False, 'use_epoch_scheduler': False, 'file_name': 'config-json-tpu-v??.txt'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pFJLyMqEglx",
        "colab_type": "text"
      },
      "source": [
        "# Define functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JNXy5AuwyuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "byte_level_tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file=os.path.join(INPUT_PATH,config.model_path,\"vocab.json\"), \n",
        "    merges_file=os.path.join(INPUT_PATH,config.model_path,\"merges.txt\"),\n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "\n",
        "def print_to_file(text, file_path=CHECKPOINT_PATH_TORCH, extension=\"txt\",display=True,file_name=None):\n",
        "    \"\"\"\n",
        "    savs the input into a file affter adding time and version information\n",
        "    \"\"\"\n",
        "    if file_name:\n",
        "        full_file_path = os.path.join(file_path, file_name)\n",
        "    else:\n",
        "        full_file_path = os.path.join(file_path, \"%s-logs.%s\"%(config.version,extension))\n",
        "    f = open(full_file_path, 'a')\n",
        "    \n",
        "\n",
        "    if type(text) == list:\n",
        "        text = \"\\t\".join([str(s) for s in text])\n",
        "    if type(text) != str:\n",
        "        text = str(text)\n",
        "\n",
        "    if display:\n",
        "        print(text)\n",
        "\n",
        "    f.write(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "    f.write(\"\\t\")\n",
        "    f.write(config.version)\n",
        "    f.write(\"\\t\")\n",
        "    f.write(text)\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "\n",
        "class Timer(object):\n",
        "    \"\"\"\n",
        "    a simple class to keep track of running time in a block.\n",
        "    code example:\n",
        "    with Timer():\n",
        "        your code\n",
        "        your code\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, name=None):\n",
        "        self.name = name\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.tstart = time.time()\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        if self.name:\n",
        "            print_to_file('[%s]' % self.name,)\n",
        "        print_to_file('Elapsed: %s' % (time.time() - self.tstart))\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "\n",
        "    a = set(str(str1).lower().split()) \n",
        "    b = set(str(str2).lower().split())\n",
        "\n",
        "    c = a.intersection(b)\n",
        "\n",
        "    if (len(a) + len(b) - len(c)) > 0:\n",
        "      return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "    else:\n",
        "      return 0\n",
        "      \n",
        "class TweetModel(transformers.BertPreTrainedModel):\n",
        "    def __init__(self, conf):\n",
        "        super(TweetModel, self).__init__(conf)\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained(os.path.join(INPUT_PATH,config.model_path), config=conf)\n",
        "        \n",
        "        if config.model_arch == \"abhishek\":\n",
        "            self.drop_out = torch.nn.Dropout(config.drop_out_1)\n",
        "            self.l0 = torch.nn.Linear(768 * 2, 2)\n",
        "            torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
        "        \n",
        "        if config.model_arch == \"abhishek_plus\":\n",
        "            self.drop_out_1 = torch.nn.Dropout(config.drop_out_1)\n",
        "            self.dense_1 = torch.nn.Linear(768 * 2, 128)\n",
        "            self.bach_norm = torch.nn.BatchNorm1d(128)\n",
        "            self.relu = torch.nn.ReLU()\n",
        "            self.dense_2 = torch.nn.Linear(128, 2)\n",
        "            torch.nn.init.normal_(self.dense_1.weight, std=0.02)\n",
        "            torch.nn.init.normal_(self.dense_2.weight, std=0.02)\n",
        "\n",
        "        if config.model_arch == \"karim\":\n",
        "            self.drop_out_1 = torch.nn.Dropout(config.drop_out_1)\n",
        "            self.conv1d1_1  = torch.nn.Conv1d(in_channels=768*2, out_channels=128, kernel_size=2)\n",
        "            self.lrelu_1    = torch.nn.LeakyReLU()\n",
        "            self.conv1d2_1  = torch.nn.Conv1d(in_channels=128, out_channels=64, kernel_size=2)\n",
        "            self.dense_1    = torch.nn.Linear(in_features=64,out_features=1)\n",
        "            self.flatten_1  = torch.nn.Flatten()\n",
        "            self.softmax_1  = torch.nn.Softmax()\n",
        "            \n",
        "            self.drop_out_2 = torch.nn.Dropout(config.drop_out_1)\n",
        "            self.conv1d1_2  = torch.nn.Conv1d(in_channels=768*2, out_channels=128, kernel_size=2)\n",
        "            self.lrelu_2    = torch.nn.LeakyReLU()\n",
        "            self.conv1d2_2  = torch.nn.Conv1d(in_channels=128, out_channels=64, kernel_size=2)\n",
        "            self.dense_2    = torch.nn.Linear(in_features=64,out_features=1)\n",
        "            self.flatten_2  = torch.nn.Flatten()\n",
        "            self.softmax_2  = torch.nn.Softmax()\n",
        "\n",
        "            # initialize weights \n",
        "            torch.nn.init.xavier_uniform_(self.conv1d1_1.weight)\n",
        "            torch.nn.init.xavier_uniform_(self.conv1d2_1.weight)\n",
        "            torch.nn.init.xavier_uniform_(self.dense_1.weight)\n",
        "            torch.nn.init.xavier_uniform_(self.conv1d1_2.weight)\n",
        "            torch.nn.init.xavier_uniform_(self.conv1d2_2.weight)\n",
        "            torch.nn.init.xavier_uniform_(self.dense_2.weight)\n",
        "            \n",
        "            # self.dense_1.bias.fill_(0.01)\n",
        "            # self.dense_2.bias.fill_(0.01)\n",
        "            # self.conv1d1_1.bias.fill_(0.01)\n",
        "            # self.conv1d2_1.bias.fill_(0.01)\n",
        "            # self.conv1d1_2.bias.fill_(0.01)\n",
        "            # self.conv1d2_2.bias.fill_(0.01)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, _, out = self.roberta(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        if config.model_arch == \"abhishek\":\n",
        "            out = torch.cat((out[-1], out[-2]), dim=-1)\n",
        "            out = self.drop_out(out)\n",
        "            logits = self.l0(out)\n",
        "            start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        if config.model_arch == \"abhishek_plus\":\n",
        "            x = torch.cat((out[-1], out[-2]), dim=-1)\n",
        "            x = self.drop_out_1(x)\n",
        "            x = self.dense_1(x)\n",
        "            # x = self.bach_norm(x)\n",
        "            x = self.relu(x)\n",
        "            logits = self.dense_2(x)\n",
        "            start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        if config.model_arch == \"karim\":\n",
        "            x = torch.cat((out[-1], out[-2]), dim=-1)\n",
        "            \n",
        "            x1 = self.drop_out_1(x)\n",
        "            x1 = x1.transpose(1,2)\n",
        "            x1 = self.conv1d1_1(x1)\n",
        "            x1 = self.lrelu_1(x1)\n",
        "            x1 = self.conv1d2_1(x1)\n",
        "            x1 = x1.transpose(1,2)\n",
        "            x1 = self.dense_1(x1)\n",
        "            x1 = self.flatten_1(x1)\n",
        "            x1 = self.softmax_1(x1)\n",
        "\n",
        "            x2 = self.drop_out_2(x)\n",
        "            x2 = x2.transpose(1,2)\n",
        "            x2 = self.conv1d1_2(x2)\n",
        "            x2 = self.lrelu_2(x2)\n",
        "            x2 = self.conv1d2_2(x2)\n",
        "            x2 = x2.transpose(1,2)\n",
        "            x2 = self.dense_2(x2)\n",
        "            x2 = self.flatten_2(x2)\n",
        "            x2 = self.softmax_2(x2)\n",
        "\n",
        "            start_logits = x1\n",
        "            end_logits = x2\n",
        "\n",
        "\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits\n",
        "\n",
        "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
        "    tweet = \" \" + \" \".join(str(tweet).split())\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
        "\n",
        "    len_st = len(selected_text) - 1\n",
        "    idx0 = None\n",
        "    idx1 = None\n",
        "\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "            idx0 = ind\n",
        "            idx1 = ind + len_st - 1\n",
        "            break\n",
        "\n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx0 != None and idx1 != None:\n",
        "        for ct in range(idx0, idx1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "    \n",
        "    tok_tweet = tokenizer.encode(tweet)\n",
        "    input_ids_orig = tok_tweet.ids\n",
        "    tweet_offsets = tok_tweet.offsets\n",
        "    \n",
        "    target_idx = []\n",
        "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset1: offset2]) > 0:\n",
        "            target_idx.append(j)\n",
        "    \n",
        "    targets_start = target_idx[0]\n",
        "    targets_end = target_idx[-1]\n",
        "\n",
        "    sentiment_id = {\n",
        "        'positive': 1313,\n",
        "        'negative': 2430,\n",
        "        'neutral': 7974\n",
        "    }\n",
        "    \n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "\n",
        "    padding_length = max_len - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        input_ids = input_ids + ([1] * padding_length)\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
        "    \n",
        "    return {\n",
        "        'ids': input_ids,\n",
        "        'mask': mask,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'targets_start': targets_start,\n",
        "        'targets_end': targets_end,\n",
        "        'orig_tweet': tweet,\n",
        "        'orig_selected': selected_text,\n",
        "        'sentiment': sentiment,\n",
        "        'offsets': tweet_offsets\n",
        "    }\n",
        "\n",
        "\n",
        "class TweetDataset:\n",
        "    def __init__(self, tweet, sentiment, selected_text):\n",
        "        self.tweet = tweet\n",
        "        self.sentiment = sentiment\n",
        "        self.selected_text = selected_text\n",
        "        self.tokenizer = byte_level_tokenizer\n",
        "        self.max_len = config.max_len\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = process_data(\n",
        "            self.tweet[item], \n",
        "            self.selected_text[item], \n",
        "            self.sentiment[item],\n",
        "            self.tokenizer,\n",
        "            self.max_len\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
        "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
        "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
        "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
        "            'orig_tweet': data[\"orig_tweet\"],\n",
        "            'orig_selected': data[\"orig_selected\"],\n",
        "            'sentiment': data[\"sentiment\"],\n",
        "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
        "        }\n",
        "def calculate_jaccard_score(\n",
        "    original_tweet, \n",
        "    target_string, \n",
        "    sentiment_val, \n",
        "    idx_start, \n",
        "    idx_end, \n",
        "    offsets,\n",
        "    verbose=False):\n",
        "    \n",
        "    if idx_end < idx_start:\n",
        "        idx_end = idx_start\n",
        "    \n",
        "    filtered_output  = \"\"\n",
        "    for ix in range(idx_start, idx_end + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
        "            filtered_output += \" \"\n",
        "\n",
        "    if len(original_tweet.split()) <= config.tweet_size_to_ignore:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "    if sentiment_val != \"neutral\" and verbose == True:\n",
        "        if filtered_output.strip().lower() != target_string.strip().lower():\n",
        "            print(\"********************************\")\n",
        "            print(f\"Output= {filtered_output.strip()}\")\n",
        "            print(f\"Target= {target_string.strip()}\")\n",
        "            print(f\"Tweet= {original_tweet.strip()}\")\n",
        "            print(\"********************************\")\n",
        "\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output\n",
        "\n",
        "\n",
        "\n",
        "class LabelSmoothLoss(torch.nn.Module):\n",
        "    \n",
        "    def __init__(self, smoothing=0.0):\n",
        "        super(LabelSmoothLoss, self).__init__()\n",
        "        self.smoothing = smoothing\n",
        "    \n",
        "    def forward(self, input, target):\n",
        "        log_prob = F.log_softmax(input, dim=-1)\n",
        "        weight = input.new_ones(input.size()) * \\\n",
        "            self.smoothing / (input.size(-1) - 1.)\n",
        "        weight.scatter_(-1, target.unsqueeze(-1), (1. - self.smoothing))\n",
        "        loss = (-weight * log_prob).sum(dim=-1).mean()\n",
        "        return loss\n",
        "\n",
        "## Alternative to implement LabelSmoothingCrossEntropy\n",
        "# def reduce_loss(loss, reduction='mean'):\n",
        "#     return loss.mean() if reduction=='mean' else loss.sum() if reduction=='sum' else loss\n",
        "\n",
        "\n",
        "# class LabelSmoothingCrossEntropy(nn.Module):\n",
        "#     def __init__(self, epsilon:float=0.1, reduction='mean'):\n",
        "#         super().__init__()\n",
        "#         self.epsilon = epsilon\n",
        "#         self.reduction = reduction\n",
        "    \n",
        "#     def forward(self, preds, target):\n",
        "#         n = preds.size()[-1]\n",
        "#         log_preds = F.log_softmax(preds, dim=-1)\n",
        "#         loss = reduce_loss(-log_preds.sum(dim=-1), self.reduction)\n",
        "#         nll = F.nll_loss(log_preds, target, reduction=self.reduction)\n",
        "#         return linear_combination(loss/n, nll, self.epsilon)\n",
        "\n",
        "labels_moothing_loss = LabelSmoothLoss(smoothing=config.get(\"smoothing_coeff\"))\n",
        "\n",
        "def pos_weight(pred_tensor, pos_tensor, neg_weight=1, pos_weight=1):\n",
        "    # neg_weight for when pred position < target position\n",
        "    # pos_weight for when pred position > target position\n",
        "    gap = torch.argmax(pred_tensor, dim=1) - pos_tensor\n",
        "    gap = gap.type(torch.float32).detach()\n",
        "    return torch.where(gap < 0, -neg_weight * gap, pos_weight * gap)\n",
        "\n",
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "\n",
        "    if config.get(\"penalty_loss\"):\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(reduce='none') # do reduction later\n",
        "        \n",
        "        start_loss = loss_fct(start_logits, start_positions) * pos_weight(start_logits, start_positions, 1, 1)\n",
        "        end_loss = loss_fct(end_logits, end_positions) * pos_weight(end_logits, end_positions, 1, 1)\n",
        "        \n",
        "        start_loss = torch.mean(start_loss)\n",
        "        end_loss = torch.mean(end_loss)\n",
        "        \n",
        "        total_loss = (start_loss + end_loss)\n",
        "        return total_loss\n",
        "\n",
        "    if (config.get(\"smoothing_coeff\")) and (config.get(\"smoothing_coeff\")>0):\n",
        "        start_loss = labels_moothing_loss.forward(start_logits, start_positions)\n",
        "        end_loss = labels_moothing_loss.forward(end_logits, end_positions)\n",
        "        total_loss = (start_loss + end_loss)\n",
        "        return total_loss\n",
        "\n",
        "    # default loss function\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "    \n",
        "    total_loss = (start_loss + end_loss)\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def get_best_start_end_idxs(start_logits, end_logits):\n",
        "    if (not config.get(\"span_choice\")) or (config.get(\"span_choice\") == \"basic\"):\n",
        "        idx_start = np.argmax(start_logits)\n",
        "        idx_end = np.argmax(end_logits)\n",
        "        return idx_start, idx_end\n",
        "\n",
        "    if (config.get(\"span_choice\") == \"smart_sum\"):\n",
        "        max_len = len(start_logits)\n",
        "        a = np.tile(start_logits, (max_len, 1))\n",
        "        b = np.tile(end_logits, (max_len, 1))\n",
        "        c = np.tril(a + b.T, k=0).T\n",
        "        c[c == 0] = -1000\n",
        "        return np.unravel_index(c.argmax(), c.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tItDUZebEcwb",
        "colab_type": "text"
      },
      "source": [
        "# Map function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY-N3sBU8GiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_fn(index, flags):\n",
        "    ## Setup \n",
        "    fold_num = flags[\"fold_num\"]\n",
        "    # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "    torch.manual_seed(6666)\n",
        "\n",
        "    # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "    device = xm.xla_device()  \n",
        "    print_to_file(str(xm.xla_real_devices([str(device)])[0]))\n",
        "\n",
        "\n",
        "    ## Dataloader construction\n",
        "    \n",
        "    train_idx = fold_ids[fold_num][0]\n",
        "    valid_idx = fold_ids[fold_num][1]\n",
        "    train_df_fold = train_df.loc[train_idx]\n",
        "    valid_df_fold = train_df.loc[valid_idx]\n",
        "\n",
        "    num_train_batches_per_core = int(train_df_fold.shape[0]/config.train_batch_size/8)\n",
        "    num_valid_batches_per_core = int(valid_df_fold.shape[0]/config.valid_batch_size/8)\n",
        "\n",
        "    train_dataset = TweetDataset(\n",
        "        tweet=train_df_fold.text.values,\n",
        "        sentiment=train_df_fold.sentiment.values,\n",
        "        selected_text=train_df_fold.selected_text.values\n",
        "    )\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=valid_df_fold.text.values,\n",
        "        sentiment=valid_df_fold.sentiment.values,\n",
        "        selected_text=valid_df_fold.selected_text.values\n",
        "    )\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "\n",
        "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    valid_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "\n",
        "    # Creates dataloaders, which load data in batches\n",
        "    # Note: test loader is not shuffled or sampled\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.train_batch_size,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=8,\n",
        "        drop_last=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.valid_batch_size,\n",
        "        sampler=test_sampler,\n",
        "        shuffle=False,\n",
        "        num_workers=8,\n",
        "        drop_last=True)\n",
        "\n",
        "\n",
        "  ## Network, optimizer, and loss function creation\n",
        "\n",
        "  #  Note: each process has its own identical copy of the model\n",
        "  #  Even though each model is created independently, they're also\n",
        "  #  created in the same way.\n",
        "\n",
        "    model_config = transformers.RobertaConfig.from_pretrained(os.path.join(INPUT_PATH,config.model_path))\n",
        "    model_config.output_hidden_states = True\n",
        "    model = TweetModel(conf=model_config).to(device)\n",
        "\n",
        "\n",
        "    if config.get('adam_type')==\"simple_adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=config.lr)\n",
        "    if config.get('adam_type')==\"type_2\":\n",
        "        optimizer = AdamW(model.parameters(), lr=config.lr, correct_bias=False)\n",
        "    if (not config.get('adam_type')) or (config.get('adam_type')==\"type_1\"):\n",
        "        param_optimizer = list(model.named_parameters())\n",
        "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_parameters = [\n",
        "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': config.scheduler_weight_decay_1},\n",
        "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': config.scheduler_weight_decay_2},\n",
        "        ]\n",
        "        optimizer = AdamW(optimizer_parameters, lr=config.lr)\n",
        "    \n",
        "    if not config.get(\"use_epoch_scheduler\"):\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, \n",
        "            num_warmup_steps=0, \n",
        "            num_training_steps=int(train_df_fold.shape[0] / config.train_batch_size * config.num_epochs / 8) #TODO: shall we divide by 8TPUs\n",
        "        )\n",
        "    else:\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer, \n",
        "            num_warmup_steps=0, \n",
        "            num_training_steps=int(config.num_epochs) #TODO: shall we divide by 8TPUs\n",
        "        )\n",
        "    ## Trains\n",
        "    train_start = time.time()\n",
        "    best_score = -10000\n",
        "    for epoch in range(config.num_epochs):\n",
        "        if xm.is_master_ordinal():\n",
        "            print_to_file(\"Epoch %i\" % (epoch+1))\n",
        "        \n",
        "        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "        if xm.is_master_ordinal():\n",
        "            batch_set = tqdm(para_train_loader, total=num_train_batches_per_core, position=0, leave=True)\n",
        "        else: \n",
        "            batch_set = para_train_loader\n",
        "        \n",
        "        for batch_num, batch in enumerate(batch_set):\n",
        "            ids = batch[\"ids\"]\n",
        "            token_type_ids = batch[\"token_type_ids\"]\n",
        "            mask = batch[\"mask\"]\n",
        "            targets_start = batch[\"targets_start\"]\n",
        "            targets_end = batch[\"targets_end\"]\n",
        "            sentiment = batch[\"sentiment\"]\n",
        "            orig_selected = batch[\"orig_selected\"]\n",
        "            orig_tweet = batch[\"orig_tweet\"]\n",
        "            targets_start = batch[\"targets_start\"]\n",
        "            targets_end = batch[\"targets_end\"]\n",
        "            offsets = batch[\"offsets\"]\n",
        "\n",
        "            model.train()\n",
        "            outputs_start, outputs_end = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "            )\n",
        "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            if config.get(\"gradient_clipping\"):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm) \n",
        "            xm.optimizer_step(optimizer)\n",
        "            if config.use_scheduler and (not config.get(\"use_epoch_scheduler\")):\n",
        "                scheduler.step()\n",
        "        xm.save(\n",
        "            model.state_dict(),\n",
        "            os.path.join(CHECKPOINT_PATH_TORCH, '%s-roberta-%i-e%i.bin'%(config.version,fold_num,epoch)),\n",
        "            master_only=True)\n",
        "        if config.get(\"use_epoch_scheduler\"):\n",
        "            scheduler.step()\n",
        "        \n",
        "        if xm.is_master_ordinal():\n",
        "            print(\"%i - lr: %.5f\" %(epoch, optimizer.state_dict()[\"param_groups\"][0][\"lr\"]))\n",
        "    # elapsed_train_time = time.time() - train_start\n",
        "    # print_to_file(\"TPU %i - finished training. Train time was: %.2fmin\" % (index, elapsed_train_time/60)) \n",
        "\n",
        "        ## Evaluation\n",
        "        model.eval()\n",
        "        eval_start = time.time()\n",
        "        jaccard_scores = []\n",
        "        valid_data_size = 0\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            para_test_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "            for batch_num, batch in enumerate(para_test_loader):\n",
        "                valid_data_size += len(batch[\"offsets\"])\n",
        "                outputs_start, outputs_end = model(\n",
        "                    ids=batch[\"ids\"],\n",
        "                    mask=batch[\"mask\"],\n",
        "                    token_type_ids=batch[\"token_type_ids\"]\n",
        "                )\n",
        "                outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
        "                outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
        "\n",
        "                for px in range(len(batch[\"offsets\"])):\n",
        "\n",
        "                    targets_start = batch[\"targets_start\"] [px]\n",
        "                    targets_end = batch[\"targets_end\"][px]\n",
        "                    offsets = batch[\"offsets\"][px]        \n",
        "\n",
        "                    idx_start, idx_end = get_best_start_end_idxs(outputs_start[px, :], outputs_end[px, :])\n",
        "\n",
        "                    jac, _ = calculate_jaccard_score(\n",
        "                        original_tweet=batch[\"orig_tweet\"][px],\n",
        "                        target_string=batch[\"orig_selected\"][px],\n",
        "                        sentiment_val=batch[\"sentiment\"][px],\n",
        "                        idx_start=idx_start,\n",
        "                        idx_end=idx_end,\n",
        "                        offsets=batch[\"offsets\"][px]\n",
        "                    )\n",
        "                    jaccard_scores.append(jac)\n",
        "        score = np.mean(jaccard_scores)\n",
        "        elapsed_eval_time = time.time() - eval_start\n",
        "        print_to_file(\"TPU%i - Evaluation time was: %.2fmin, jaccard average: %.5f\" % (index, elapsed_eval_time/60, score))\n",
        "        print_to_file(\n",
        "            [\n",
        "             'F%i'%fold_num,\n",
        "             \"E%i\"%(epoch),\n",
        "             \"%.5f\"%score,\n",
        "             'TPU%i'%index,\n",
        "             \"#%i\"%(valid_data_size),\n",
        "             ],\n",
        "             file_path=CHECKPOINT_PATH_HP_TUNING,\n",
        "             extension=\"csv\",\n",
        "             display=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnig7RnUtL4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if config.dev:\n",
        "    train_df = pd.read_csv(DATA_PATH + 'train.csv').dropna().sample(n=3000, random_state=888).reset_index()\n",
        "else:\n",
        "    train_df = pd.read_csv(DATA_PATH + 'train.csv').dropna().reset_index()\n",
        "\n",
        "fold_ids = []\n",
        "\n",
        "kfold = model_selection.KFold(n_splits=config.num_folds, shuffle=True, random_state=42)\n",
        "for fold_num, (train_idx, valid_idx) in enumerate(kfold.split(train_df.text)):\n",
        "    fold_ids.append((train_idx, valid_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnAv0qfTEWje",
        "colab_type": "text"
      },
      "source": [
        "# Run K-Fold Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdD9YTodNPEe",
        "colab_type": "code",
        "outputId": "e263efb9-ae5a-47c4-d298-1dd9ac11d447",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "print(\"VERSION: \", config.version)\n",
        "for fold_num in range(config.num_folds_to_train):\n",
        "    print_to_file(\"Starting Training for Fold %i\"%(fold_num+1))\n",
        "    flags = {\"fold_num\": fold_num}\n",
        "    xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VERSION:  tpu-v28\n",
            "Starting Training for Fold 1\n",
            "TPU:0\n",
            "TPU:1\n",
            "TPU:4\n",
            "TPU:3\n",
            "TPU:5\n",
            "TPU:6\n",
            "TPU:2\n",
            "TPU:7\n",
            "Epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 343/343 [01:09<00:00,  4.94it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 - lr: 0.00003\n",
            "TPU5 - Evaluation time was: 1.07min, jaccard average: 0.70468\n",
            "TPU1 - Evaluation time was: 1.10min, jaccard average: 0.67076\n",
            "TPU2 - Evaluation time was: 1.12min, jaccard average: 0.71228\n",
            "TPU4 - Evaluation time was: 1.15min, jaccard average: 0.70550\n",
            "TPU7 - Evaluation time was: 1.16min, jaccard average: 0.71628\n",
            "TPU3 - Evaluation time was: 1.19min, jaccard average: 0.68780\n",
            "TPU6 - Evaluation time was: 1.25min, jaccard average: 0.71258\n",
            "TPU0 - Evaluation time was: 1.26min, jaccard average: 0.74106\n",
            "Epoch 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 343/343 [00:57<00:00,  5.95it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 - lr: 0.00002\n",
            "TPU5 - Evaluation time was: 1.05min, jaccard average: 0.70863\n",
            "TPU1 - Evaluation time was: 1.05min, jaccard average: 0.68066\n",
            "TPU2 - Evaluation time was: 1.09min, jaccard average: 0.72544\n",
            "TPU4 - Evaluation time was: 1.12min, jaccard average: 0.71508\n",
            "TPU7 - Evaluation time was: 1.12min, jaccard average: 0.71739\n",
            "TPU3 - Evaluation time was: 1.18min, jaccard average: 0.69690\n",
            "TPU6 - Evaluation time was: 1.23min, jaccard average: 0.71542\n",
            "TPU0 - Evaluation time was: 1.24min, jaccard average: 0.75261\n",
            "Epoch 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 11%|█         | 37/343 [00:07<00:50,  6.02it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBSfl_FhIRHF",
        "colab_type": "text"
      },
      "source": [
        "# Get Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-HBpf88kRqp",
        "colab_type": "code",
        "outputId": "793c071d-6981-46a0-ae81-cb91598f4989",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        }
      },
      "source": [
        "file_path = os.path.join(CHECKPOINT_PATH_HP_TUNING,\"%s-logs.csv\"%config.version)\n",
        "hp_df = pd.read_csv(file_path,sep=\"\\t\",header=None)\n",
        "hp_df.columns = [\"datetime\",\"version\",\"fold\",\"epoch\",\"score\",\"tpu\", \"size\"]\n",
        "print(file_path)\n",
        "print(hp_df.shape)\n",
        "hp_df.head(10)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/input/model_checkpoint/roberta/hp-tuning/tpu-v27-logs.csv\n",
            "(48, 7)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>datetime</th>\n",
              "      <th>version</th>\n",
              "      <th>fold</th>\n",
              "      <th>epoch</th>\n",
              "      <th>score</th>\n",
              "      <th>tpu</th>\n",
              "      <th>size</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20200503-065530</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E0</td>\n",
              "      <td>0.48229</td>\n",
              "      <td>TPU7</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20200503-065532</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E0</td>\n",
              "      <td>0.52039</td>\n",
              "      <td>TPU1</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20200503-065532</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E0</td>\n",
              "      <td>0.50699</td>\n",
              "      <td>TPU5</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20200503-065533</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E0</td>\n",
              "      <td>0.49503</td>\n",
              "      <td>TPU6</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20200503-065533</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E0</td>\n",
              "      <td>0.45083</td>\n",
              "      <td>TPU3</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>20200503-065534</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E0</td>\n",
              "      <td>0.54176</td>\n",
              "      <td>TPU4</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>20200503-065534</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E0</td>\n",
              "      <td>0.44039</td>\n",
              "      <td>TPU0</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>20200503-065534</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E0</td>\n",
              "      <td>0.49243</td>\n",
              "      <td>TPU2</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>20200503-065552</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E1</td>\n",
              "      <td>0.57880</td>\n",
              "      <td>TPU1</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>20200503-065552</td>\n",
              "      <td>tpu-v27</td>\n",
              "      <td>F0</td>\n",
              "      <td>E1</td>\n",
              "      <td>0.58364</td>\n",
              "      <td>TPU7</td>\n",
              "      <td>#64</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          datetime  version fold epoch    score   tpu size\n",
              "0  20200503-065530  tpu-v27   F0    E0  0.48229  TPU7  #64\n",
              "1  20200503-065532  tpu-v27   F0    E0  0.52039  TPU1  #64\n",
              "2  20200503-065532  tpu-v27   F0    E0  0.50699  TPU5  #64\n",
              "3  20200503-065533  tpu-v27   F0    E0  0.49503  TPU6  #64\n",
              "4  20200503-065533  tpu-v27   F0    E0  0.45083  TPU3  #64\n",
              "5  20200503-065534  tpu-v27   F0    E0  0.54176  TPU4  #64\n",
              "6  20200503-065534  tpu-v27   F0    E0  0.44039  TPU0  #64\n",
              "7  20200503-065534  tpu-v27   F0    E0  0.49243  TPU2  #64\n",
              "8  20200503-065552  tpu-v27   F0    E1  0.57880  TPU1  #64\n",
              "9  20200503-065552  tpu-v27   F0    E1  0.58364  TPU7  #64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6ctOS0Cj9e7",
        "colab_type": "code",
        "outputId": "549d18f8-aff7-40ff-bb91-6da727fb49fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "hp_df_agg = hp_df.groupby([\"version\",\"fold\",\"epoch\"]).score.mean().reset_index()\n",
        "best_models = []\n",
        "best_scores = []\n",
        "\n",
        "for f in hp_df_agg.fold.unique():\n",
        "    best_epoch = hp_df_agg[hp_df_agg.fold==f].sort_values(by=\"score\", ascending=False).epoch.iloc[0]\n",
        "    best_score = hp_df_agg[hp_df_agg.fold==f].sort_values(by=\"score\", ascending=False).score.iloc[0]\n",
        "    best_model = \"%s-roberta-%s-e%s.bin\"%(config.version,f[1], best_epoch[1])\n",
        "    print (best_model, f, best_epoch, \"%.5f\"%best_score)\n",
        "    best_scores.append(best_score)\n",
        "    best_models.append(best_model)\n",
        "print(\"overall_score: \", np.mean(best_scores))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tpu-v27-roberta-0-e2.bin F0 E2 0.65224\n",
            "overall_score:  0.652238125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkpuFmxroLS2",
        "colab_type": "text"
      },
      "source": [
        "# Logging summary and cleaning up the models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIfAt71EmKDm",
        "colab_type": "code",
        "outputId": "6ee13301-4e0e-4f59-fa32-8a8e8aec08cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "files_to_remove = []\n",
        "for f in os.listdir(CHECKPOINT_PATH_TORCH):\n",
        "    if (f.startswith(config.version)) and (f.endswith(\".bin\")):\n",
        "        if f in best_models:\n",
        "            print(f, \"good model\")\n",
        "        else:\n",
        "            print(f, \" will be deleted.\")\n",
        "            files_to_remove.append(f)\n",
        "\n",
        "if len(files_to_remove)>0:\n",
        "    print(\"preparing to delete %i files\"%len(files_to_remove))\n",
        "    time.sleep(5)\n",
        "    for f in files_to_remove:\n",
        "        os.remove(os.path.join(CHECKPOINT_PATH_TORCH,f))\n",
        "    print(\"deletion complete\")    "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tpu-v27-roberta-0-e0.bin  will be deleted.\n",
            "tpu-v27-roberta-0-e1.bin  will be deleted.\n",
            "tpu-v27-roberta-0-e2.bin good model\n",
            "preparing to delete 2 files\n",
            "deletion complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qtsuWCap1eU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print_to_file(\n",
        "    [\n",
        "     config.version,\n",
        "     np.mean(best_scores),\n",
        "     len(best_scores)\n",
        "     ],\n",
        "     file_path=CHECKPOINT_PATH_HP_TUNING,\n",
        "     extension=\"csv\",\n",
        "     display=True,\n",
        "     file_name=\"models_summary.csv\",\n",
        "     )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}