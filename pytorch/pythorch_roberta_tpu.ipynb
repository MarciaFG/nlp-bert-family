{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pythorch_roberta_tpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMZEJe5xHx1aqulYPIPl48Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/branjbar/nlp-bert-family/blob/master/pytorch/pythorch_roberta_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN-jcaOwcSls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CHANGES:\n",
        "## lr increase\n",
        "## TRAIN_BATCH_SIZE\n",
        "## VALID_BATCH_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt_o8YchvJ98",
        "colab_type": "code",
        "outputId": "bfec1962-909e-41c7-cb28-5aac83e58b34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To avoid notebook get disconnected\n",
        "\"\"\"\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button\").click() \n",
        "}setInterval(ClickConnect,60000)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "caffeinate\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncaffeinate\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teCsiKGxyoPo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODOS:\n",
        "## add tic and toc to calculate the total time!\n",
        "## check the pretrained model\n",
        "## remove neutral from the filtering"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgnUUTnjxCOb",
        "colab_type": "code",
        "outputId": "ed6d482e-d1bc-4ce4-a359-52e1cd206163",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvblIFXV_iJe",
        "colab_type": "code",
        "outputId": "cf9d7b56-2690-4774-e317-7c2ca4cb8335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "# Installs PyTorch, PyTorch/XLA, and Torchvision\n",
        "# Copy this cell into your own notebooks to use PyTorch on Cloud TPUs \n",
        "# Warning: this may take a couple minutes to run\n",
        "\n",
        "import os\n",
        "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
        "\n",
        "VERSION = \"20200325\"  # @ param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r 50  3727   50  1874    0     0  11567      0 --:--:-- --:--:-- --:--:-- 11496\r100  3727  100  3727    0     0  23006      0 --:--:-- --:--:-- --:--:-- 22865\n",
            "Updating TPU and VM. This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-dev20200325 ...\n",
            "Uninstalling torch-1.5.0a0+d6149a7:\n",
            "  Successfully uninstalled torch-1.5.0a0+d6149a7\n",
            "Uninstalling torchvision-0.6.0a0+3c254fb:\n",
            "  Successfully uninstalled torchvision-0.6.0a0+3c254fb\n",
            "Copying gs://tpu-pytorch/wheels/torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "| [1 files][ 83.4 MiB/ 83.4 MiB]                                                \n",
            "Operation completed over 1 objects/83.4 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][114.5 MiB/114.5 MiB]                                                \n",
            "Operation completed over 1 objects/114.5 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (1.18.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200325) (0.16.0)\n",
            "\u001b[31mERROR: fastai 1.0.60 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Done updating TPU runtime: <Response [200]>\n",
            "Successfully installed torch-1.5.0a0+d6149a7\n",
            "Processing ./torch_xla-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Installing collected packages: torch-xla\n",
            "  Found existing installation: torch-xla 1.6+e788e5b\n",
            "    Uninstalling torch-xla-1.6+e788e5b:\n",
            "      Successfully uninstalled torch-xla-1.6+e788e5b\n",
            "Successfully installed torch-xla-1.6+e788e5b\n",
            "Processing ./torchvision-nightly+20200325-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.5.0a0+d6149a7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.18.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200325) (1.12.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200325) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp5 is already the newest version (5.0.1-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WShSfuy67F0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM-eZntGvP6E",
        "colab_type": "code",
        "outputId": "9859795b-66b8-4136-cee9-6065345b8e28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!pip install transformers\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tokenizers\n",
        "import string\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "from sklearn import model_selection\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.46)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.46)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqU8gplkvcHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KAGGLE_INPUT_PATH = \"../input/\"\n",
        "COLAB_INPUT_PATH = \"/content/drive/My Drive/input/\"\n",
        "INPUT_PATH = COLAB_INPUT_PATH\n",
        "\n",
        "DATA_PATH = INPUT_PATH + \"tweet-sentiment-extraction/\"\n",
        "CHECKPOINT_PATH = INPUT_PATH + \"model_checkpoint/roberta/\"\n",
        "CHECKPOINT_PATH_TORCH = INPUT_PATH + \"model_checkpoint/roberta/torch/\"\n",
        "# ROBERTA_PATH = INPUT_PATH + 'models/output_twitter/'\n",
        "ROBERTA_PATH = INPUT_PATH + 'models/roberta-base/'\n",
        "\n",
        "VER='tpu-v1'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AIiU0Wt_ZPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_to_file(text):\n",
        "  try:\n",
        "    f = open(CHECKPOINT_PATH_TORCH + VER + \"-logs.txt\", 'a')\n",
        "  except:\n",
        "    f = open(CHECKPOINT_PATH_TORCH + VER + \"-logs.txt\", 'w')\n",
        "\n",
        "  print(text)\n",
        "  f.write(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  f.write(\"  -- \")\n",
        "  f.write(text)\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "class Timer(object):\n",
        "    def __init__(self, name=None):\n",
        "        self.name = name\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.tstart = time.time()\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        if self.name:\n",
        "            print_to_file('[%s]' % self.name,)\n",
        "        print_to_file('Elapsed: %s' % (time.time() - self.tstart))\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "\n",
        "    a = set(str(str1).lower().split()) \n",
        "    b = set(str(str2).lower().split())\n",
        "\n",
        "    c = a.intersection(b)\n",
        "\n",
        "    if (len(a) + len(b) - len(c)) > 0:\n",
        "      return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36CILOlkvNXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class config():\n",
        "    MAX_LEN = 192\n",
        "    TRAIN_BATCH_SIZE = 64\n",
        "    VALID_BATCH_SIZE = 32\n",
        "    EPOCHS = 3\n",
        "    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n",
        "        vocab_file=f\"{ROBERTA_PATH}/vocab.json\", \n",
        "        merges_file=f\"{ROBERTA_PATH}/merges.txt\", \n",
        "        lowercase=True,\n",
        "        add_prefix_space=True\n",
        "    )\n",
        "    LR = 4e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JNXy5AuwyuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TweetModel(transformers.BertPreTrainedModel):\n",
        "    def __init__(self, conf):\n",
        "        super(TweetModel, self).__init__(conf)\n",
        "        self.roberta = transformers.RobertaModel.from_pretrained(ROBERTA_PATH, config=conf)\n",
        "        self.drop_out = nn.Dropout(0.1)\n",
        "        self.l0 = nn.Linear(768 * 2, 2)\n",
        "        torch.nn.init.normal_(self.l0.weight, std=0.02)\n",
        "    \n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, _, out = self.roberta(\n",
        "            ids,\n",
        "            attention_mask=mask,\n",
        "            token_type_ids=token_type_ids\n",
        "        )\n",
        "\n",
        "        out = torch.cat((out[-1], out[-2]), dim=-1)\n",
        "        out = self.drop_out(out)\n",
        "        logits = self.l0(out)\n",
        "\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        return start_logits, end_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br0GKWZ9w2M-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(tweet, selected_text, sentiment, tokenizer, max_len):\n",
        "    tweet = \" \" + \" \".join(str(tweet).split())\n",
        "    selected_text = \" \" + \" \".join(str(selected_text).split())\n",
        "\n",
        "    len_st = len(selected_text) - 1\n",
        "    idx0 = None\n",
        "    idx1 = None\n",
        "\n",
        "    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
        "        if \" \" + tweet[ind: ind+len_st] == selected_text:\n",
        "            idx0 = ind\n",
        "            idx1 = ind + len_st - 1\n",
        "            break\n",
        "\n",
        "    char_targets = [0] * len(tweet)\n",
        "    if idx0 != None and idx1 != None:\n",
        "        for ct in range(idx0, idx1 + 1):\n",
        "            char_targets[ct] = 1\n",
        "    \n",
        "    tok_tweet = tokenizer.encode(tweet)\n",
        "    input_ids_orig = tok_tweet.ids\n",
        "    tweet_offsets = tok_tweet.offsets\n",
        "    \n",
        "    target_idx = []\n",
        "    for j, (offset1, offset2) in enumerate(tweet_offsets):\n",
        "        if sum(char_targets[offset1: offset2]) > 0:\n",
        "            target_idx.append(j)\n",
        "    \n",
        "    targets_start = target_idx[0]\n",
        "    targets_end = target_idx[-1]\n",
        "\n",
        "    sentiment_id = {\n",
        "        'positive': 1313,\n",
        "        'negative': 2430,\n",
        "        'neutral': 7974\n",
        "    }\n",
        "    \n",
        "    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n",
        "    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n",
        "    mask = [1] * len(token_type_ids)\n",
        "    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n",
        "    targets_start += 4\n",
        "    targets_end += 4\n",
        "\n",
        "    padding_length = max_len - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        input_ids = input_ids + ([1] * padding_length)\n",
        "        mask = mask + ([0] * padding_length)\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\n",
        "        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n",
        "    \n",
        "    return {\n",
        "        'ids': input_ids,\n",
        "        'mask': mask,\n",
        "        'token_type_ids': token_type_ids,\n",
        "        'targets_start': targets_start,\n",
        "        'targets_end': targets_end,\n",
        "        'orig_tweet': tweet,\n",
        "        'orig_selected': selected_text,\n",
        "        'sentiment': sentiment,\n",
        "        'offsets': tweet_offsets\n",
        "    }\n",
        "\n",
        "\n",
        "class TweetDataset:\n",
        "    def __init__(self, tweet, sentiment, selected_text):\n",
        "        self.tweet = tweet\n",
        "        self.sentiment = sentiment\n",
        "        self.selected_text = selected_text\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.tweet)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = process_data(\n",
        "            self.tweet[item], \n",
        "            self.selected_text[item], \n",
        "            self.sentiment[item],\n",
        "            self.tokenizer,\n",
        "            self.max_len\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
        "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
        "            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
        "            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
        "            'orig_tweet': data[\"orig_tweet\"],\n",
        "            'orig_selected': data[\"orig_selected\"],\n",
        "            'sentiment': data[\"sentiment\"],\n",
        "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y95k4WQbw4c-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_jaccard_score(\n",
        "    original_tweet, \n",
        "    target_string, \n",
        "    sentiment_val, \n",
        "    idx_start, \n",
        "    idx_end, \n",
        "    offsets,\n",
        "    verbose=False):\n",
        "    \n",
        "    if idx_end < idx_start:\n",
        "        idx_end = idx_start\n",
        "    \n",
        "    filtered_output  = \"\"\n",
        "    for ix in range(idx_start, idx_end + 1):\n",
        "        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n",
        "        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
        "            filtered_output += \" \"\n",
        "\n",
        "    if len(original_tweet.split()) < 2:\n",
        "        filtered_output = original_tweet\n",
        "\n",
        "    if sentiment_val != \"neutral\" and verbose == True:\n",
        "        if filtered_output.strip().lower() != target_string.strip().lower():\n",
        "            print(\"********************************\")\n",
        "            print(f\"Output= {filtered_output.strip()}\")\n",
        "            print(f\"Target= {target_string.strip()}\")\n",
        "            print(f\"Tweet= {original_tweet.strip()}\")\n",
        "            print(\"********************************\")\n",
        "\n",
        "    jac = jaccard(target_string.strip(), filtered_output.strip())\n",
        "    return jac, filtered_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrgoIALpCJIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
        "    loss_fct = nn.CrossEntropyLoss()\n",
        "    start_loss = loss_fct(start_logits, start_positions)\n",
        "    end_loss = loss_fct(end_logits, end_positions)\n",
        "    total_loss = (start_loss + end_loss)\n",
        "    return total_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj0l_qwbw6de",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(DATA_PATH + 'train.csv').dropna().reset_index()#.sample(n=2000).reset_index()\n",
        "\n",
        "df_test = pd.read_csv(DATA_PATH + 'test.csv')\n",
        "df_test.loc[:, \"selected_text\"] = df_test.text.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0XDggKBcsdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fold_ids = []\n",
        "num_folds = 5\n",
        "\n",
        "kfold = model_selection.KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
        "for fold_num, (train_idx, valid_idx) in enumerate(kfold.split(train_df.text)):\n",
        "    fold_ids.append((train_idx, valid_idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY-N3sBU8GiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def map_fn(index, flags):\n",
        "  ## Setup \n",
        "    fold_num = flags[\"fold_num\"]\n",
        "    # Sets a common random seed - both for initialization and ensuring graph is the same\n",
        "    torch.manual_seed(6666)\n",
        "\n",
        "    # Acquires the (unique) Cloud TPU core corresponding to this process's index\n",
        "    device = xm.xla_device()  \n",
        "    print_to_file(\"TPU in use:\" + str(xm.xla_real_devices([str(device)])[0]))\n",
        "\n",
        "\n",
        "    ## Dataloader construction\n",
        "    \n",
        "    train_idx = fold_ids[fold_num][0]\n",
        "    valid_idx = fold_ids[fold_num][1]\n",
        "    train_df_fold = train_df.loc[train_idx]\n",
        "    valid_df_fold = train_df.loc[valid_idx]\n",
        "\n",
        "    num_train_batches_per_core = int(train_df_fold.shape[0]/config.TRAIN_BATCH_SIZE/8)\n",
        "    num_valid_batches_per_core = int(valid_df_fold.shape[0]/config.VALID_BATCH_SIZE/8)\n",
        "\n",
        "    train_dataset = TweetDataset(\n",
        "        tweet=train_df_fold.text.values,\n",
        "        sentiment=train_df_fold.sentiment.values,\n",
        "        selected_text=train_df_fold.selected_text.values\n",
        "    )\n",
        "\n",
        "    valid_dataset = TweetDataset(\n",
        "        tweet=valid_df_fold.text.values,\n",
        "        sentiment=valid_df_fold.sentiment.values,\n",
        "        selected_text=valid_df_fold.selected_text.values\n",
        "    )\n",
        "\n",
        "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    train_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=True)\n",
        "\n",
        "    test_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "    valid_dataset,\n",
        "    num_replicas=xm.xrt_world_size(),\n",
        "    rank=xm.get_ordinal(),\n",
        "    shuffle=False)\n",
        "\n",
        "    # Creates dataloaders, which load data in batches\n",
        "    # Note: test loader is not shuffled or sampled\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        sampler=train_sampler,\n",
        "        num_workers=8,\n",
        "        drop_last=True)\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        sampler=test_sampler,\n",
        "        shuffle=False,\n",
        "        num_workers=8,\n",
        "        drop_last=True)\n",
        "\n",
        "\n",
        "  ## Network, optimizer, and loss function creation\n",
        "\n",
        "  #  Note: each process has its own identical copy of the model\n",
        "  #  Even though each model is created independently, they're also\n",
        "  #  created in the same way.\n",
        "\n",
        "    model_config = transformers.RobertaConfig.from_pretrained(ROBERTA_PATH)\n",
        "    model_config.output_hidden_states = True\n",
        "    model = TweetModel(conf=model_config).to(device).train()\n",
        "\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "    optimizer_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_parameters, lr=config.LR)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps=0, \n",
        "        num_training_steps=int(train_df_fold.shape[0] / config.TRAIN_BATCH_SIZE * config.EPOCHS) #TODO: shall we divide by 8TPUs\n",
        "    )\n",
        "\n",
        "    ## Trains\n",
        "    train_start = time.time()\n",
        "    for epoch in range(config.EPOCHS):\n",
        "        if xm.is_master_ordinal():\n",
        "            print(\"starting epoch %i on core %i\" % (epoch, index))\n",
        "        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "\n",
        "        if xm.is_master_ordinal():\n",
        "            batch_set = tqdm(para_train_loader, total=num_train_batches_per_core, position=0, leave=True)\n",
        "        else: \n",
        "            batch_set = para_train_loader\n",
        "        for batch_num, batch in enumerate(batch_set):\n",
        "            ids = batch[\"ids\"]\n",
        "            token_type_ids = batch[\"token_type_ids\"]\n",
        "            mask = batch[\"mask\"]\n",
        "            targets_start = batch[\"targets_start\"]\n",
        "            targets_end = batch[\"targets_end\"]\n",
        "            sentiment = batch[\"sentiment\"]\n",
        "            orig_selected = batch[\"orig_selected\"]\n",
        "            orig_tweet = batch[\"orig_tweet\"]\n",
        "            targets_start = batch[\"targets_start\"]\n",
        "            targets_end = batch[\"targets_end\"]\n",
        "            offsets = batch[\"offsets\"]\n",
        "\n",
        "            outputs_start, outputs_end = model(\n",
        "                ids=ids,\n",
        "                mask=mask,\n",
        "                token_type_ids=token_type_ids,\n",
        "            )\n",
        "            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)\n",
        "            scheduler.step()\n",
        "\n",
        "    elapsed_train_time = time.time() - train_start\n",
        "    print_to_file(\"TPU %i - finished training. Train time was: %.2fmin\" % (index, elapsed_train_time/60)) \n",
        "\n",
        "\n",
        "    ## Evaluation\n",
        "    model.eval()\n",
        "    eval_start = time.time()\n",
        "    with torch.no_grad():\n",
        "\n",
        "        final_output = []\n",
        "        jaccard_scores = []\n",
        "        para_test_loader = pl.ParallelLoader(test_loader, [device]).per_device_loader(device)\n",
        "        if xm.is_master_ordinal():\n",
        "            batch_set = tqdm(para_test_loader, total=num_valid_batches_per_core, position=0, leave=True)\n",
        "        else: \n",
        "            batch_set = para_test_loader\n",
        "        for batch_num, batch in enumerate(batch_set):\n",
        "            outputs_start, outputs_end = model(\n",
        "                ids=batch[\"ids\"],\n",
        "                mask=batch[\"mask\"],\n",
        "                token_type_ids=batch[\"token_type_ids\"]\n",
        "            )\n",
        "            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n",
        "            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n",
        "\n",
        "            for px in range(len(batch[\"offsets\"])):\n",
        "\n",
        "                targets_start = batch[\"targets_start\"] [px]\n",
        "                targets_end = batch[\"targets_end\"][px]\n",
        "                offsets = batch[\"offsets\"][px]        \n",
        "                \n",
        "                jac, output_sentence = calculate_jaccard_score(\n",
        "                    original_tweet=batch[\"orig_tweet\"][px],\n",
        "                    target_string=batch[\"orig_selected\"][px],\n",
        "                    sentiment_val=batch[\"sentiment\"][px],\n",
        "                    idx_start=np.argmax(outputs_start[px, :]),\n",
        "                    idx_end=np.argmax(outputs_end[px, :]),\n",
        "                    offsets=batch[\"offsets\"][px]\n",
        "                )\n",
        "                final_output.append(output_sentence)\n",
        "                jaccard_scores.append(jac)\n",
        "        jacc_total[fold_num][index][2] = np.mean(jaccard_scores)\n",
        "    xm.save(model.state_dict(), CHECKPOINT_PATH_TORCH + '%s-roberta-%i.bin'%(VER,fold_num), master_only=True)\n",
        "    elapsed_eval_time = time.time() - eval_start\n",
        "    print_to_file(\"TPU %i - Evaluation time was: %.2fmin, jaccard average: %.5f\" % (index, elapsed_eval_time/60, np.mean(jaccard_scores)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBDd03eF-D5H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5dd574cc-dbdb-4bb1-cbb4-714f66f300ac"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Configures training (and evaluation) parameters\n",
        "\n",
        "jacc_total = {}\n",
        "for fold_num in range(5):\n",
        "    jacc_total[fold_num] = {}\n",
        "    for tpu_core in range(8):\n",
        "        jacc_total[fold_num][tpu_core] = {}\n",
        "        for epoch_num in range(8):\n",
        "            jacc_total[fold_num][tpu_core][epoch_num] = 0\n",
        "            \n",
        "for fold_num in range(5):\n",
        "    print_to_file(\"Starting Training for Fold %i\"%fold_num)\n",
        "    flags = {\"fold_num\": fold_num}\n",
        "    xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training for Fold 0\n",
            "TPU in use:TPU:0\n",
            "TPU in use:TPU:4\n",
            "TPU in use:TPU:1\n",
            "TPU in use:TPU:7\n",
            "TPU in use:TPU:5\n",
            "TPU in use:TPU:6\n",
            "TPU in use:TPU:3\n",
            "TPU in use:TPU:2\n",
            "starting epoch 0 on core 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [01:31<00:00,  2.17s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting epoch 1 on core 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:31<00:00,  1.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting epoch 2 on core 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:31<00:00,  1.31it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU 1 - finished training. Train time was: 2.50min\n",
            "TPU 4 - finished training. Train time was: 2.52min\n",
            "TPU 7 - finished training. Train time was: 2.49min\n",
            "TPU 6 - finished training. Train time was: 2.45min\n",
            "TPU 5 - finished training. Train time was: 2.46min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 42/42 [00:31<00:00,  1.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU 0 - finished training. Train time was: 2.59min\n",
            "TPU 2 - finished training. Train time was: 2.46min\n",
            "TPU 3 - finished training. Train time was: 2.46min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [01:43<00:00,  4.91s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU 3 - Evaluation time was: 1.78min, jaccard average: 0.67914\n",
            "TPU 5 - Evaluation time was: 1.78min, jaccard average: 0.69373\n",
            "TPU 7 - Evaluation time was: 1.78min, jaccard average: 0.71991\n",
            "TPU 2 - Evaluation time was: 1.78min, jaccard average: 0.72072\n",
            "TPU 1 - Evaluation time was: 1.78min, jaccard average: 0.67401\n",
            "TPU 6 - Evaluation time was: 1.78min, jaccard average: 0.70940\n",
            "TPU 4 - Evaluation time was: 1.78min, jaccard average: 0.69388\n",
            "TPU 0 - Evaluation time was: 1.78min, jaccard average: 0.72546\n",
            "Starting Training for Fold 1\n",
            "TPU in use:TPU:0\n",
            "starting epoch 0 on core 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  2%|▏         | 1/42 [00:00<00:18,  2.19it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU in use:TPU:6\n",
            "TPU in use:TPU:4\n",
            "TPU in use:TPU:5\n",
            "TPU in use:TPU:3\n",
            "TPU in use:TPU:1\n",
            "TPU in use:TPU:7\n",
            "TPU in use:TPU:2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [01:11<00:00,  1.70s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting epoch 1 on core 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:31<00:00,  1.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting epoch 2 on core 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:32<00:00,  1.32it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU 7 - finished training. Train time was: 1.64min\n",
            "TPU 5 - finished training. Train time was: 1.66min\n",
            "TPU 4 - finished training. Train time was: 1.72min\n",
            "TPU 2 - finished training. Train time was: 1.61min\n",
            "TPU 3 - finished training. Train time was: 1.64min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 42/42 [00:32<00:00,  1.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU 0 - finished training. Train time was: 2.26min\n",
            "TPU 1 - finished training. Train time was: 1.63min\n",
            "TPU 6 - finished training. Train time was: 1.98min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 21/21 [01:18<00:00,  3.72s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU 7 - Evaluation time was: 1.41min, jaccard average: 0.70342\n",
            "TPU 4 - Evaluation time was: 1.41min, jaccard average: 0.69460\n",
            "TPU 3 - Evaluation time was: 1.41min, jaccard average: 0.67402\n",
            "TPU 2 - Evaluation time was: 1.41min, jaccard average: 0.67973\n",
            "TPU 1 - Evaluation time was: 1.41min, jaccard average: 0.69290\n",
            "TPU 5 - Evaluation time was: 1.41min, jaccard average: 0.66784\n",
            "TPU 6 - Evaluation time was: 1.41min, jaccard average: 0.70128\n",
            "TPU 0 - Evaluation time was: 1.41min, jaccard average: 0.69336\n",
            "Starting Training for Fold 2\n",
            "TPU in use:TPU:0\n",
            "TPU in use:TPU:1\n",
            "TPU in use:TPU:7\n",
            "TPU in use:TPU:2\n",
            "TPU in use:TPU:4\n",
            "TPU in use:TPU:5\n",
            "TPU in use:TPU:6\n",
            "TPU in use:TPU:3\n",
            "starting epoch 0 on core 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:37<00:00,  1.12it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting epoch 1 on core 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:32<00:00,  1.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "starting epoch 2 on core 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 42/42 [00:31<00:00,  1.33it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU 6 - finished training. Train time was: 1.64min\n",
            "TPU 7 - finished training. Train time was: 1.64min\n",
            "TPU 1 - finished training. Train time was: 1.69min\n",
            "TPU 2 - finished training. Train time was: 1.64min\n",
            "TPU 3 - finished training. Train time was: 1.62min\n",
            "TPU 4 - finished training. Train time was: 1.64min\n",
            "TPU 5 - finished training. Train time was: 1.65min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 42/42 [00:31<00:00,  1.31it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TPU 0 - finished training. Train time was: 1.70min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/21 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sbd7505EiOy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}