{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_roberta_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNoni1kImfS4ALVbpz4njRX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/branjbar/nlp-bert-family/blob/master/tensorflow/tensorflow_roberta_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-2ew8z1lmev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read to use TPU https://blog.tensorflow.org/2019/01/keras-on-tpus-in-colab.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okm4ne1CV7av",
        "colab_type": "code",
        "outputId": "aa461380-3152-4b56-edc9-512498902612",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To avoid notebook get disconnected\n",
        "\"\"\"\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button\").click() \n",
        "}setInterval(ClickConnect,60000)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "caffeinate\n",
        "\"\"\""
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncaffeinate\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VmEWu6QwCLX",
        "colab_type": "code",
        "outputId": "ec031387-2cde-41d5-e6b6-72aadc022f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Sat May  2 18:09:35 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    31W / 250W |   1413MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.7 gigabytes of available RAM\n",
            "\n",
            "To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"\n",
            "menu, and then select High-RAM in the Runtime shape dropdown. Then, \n",
            "re-execute this cell.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGTHRzFTwKSI",
        "colab_type": "code",
        "outputId": "c9074328-e612-4d77-d2d2-8619f7465ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install transformers\n",
        "import os\n",
        "import pandas as pd, numpy as np\n",
        "import time, re, math\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "print('TF version',tf.__version__)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "TF version 2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adF-Or6QwwQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KAGGLE_INPUT_PATH = \"../input/\"\n",
        "COLAB_INPUT_PATH = \"/content/drive/My Drive/input/\"\n",
        "INPUT_PATH = COLAB_INPUT_PATH\n",
        "\n",
        "DATA_PATH = INPUT_PATH + \"tweet-sentiment-extraction/\"\n",
        "CHECKPOINT_PATH = INPUT_PATH + \"model_checkpoint/roberta/\"\n",
        "ROBERTA_PATH = INPUT_PATH + 'tf-roberta/'\n",
        "\n",
        "VER='v8'\n",
        "DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "\n",
        "DEV_MODE = False # if True only looks at a subset of data\n",
        "MAX_SUM_PROB = False "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2TdPHvh4XdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_html(text):\n",
        "    text = re.sub(\"&quot;\", '\"', text)\n",
        "    text = re.sub(\"&gt;\", \">\", text)\n",
        "    text = re.sub(\"&lt;\", \"<\", text)\n",
        "    text = re.sub(\"&le;\", \"≤\", text)\n",
        "    text = re.sub(\"&ge;\", \"≥\", text)\n",
        "    text = re.sub(\"&amp;\", \"&\", text)\n",
        "    return text\n",
        "\n",
        "def add_html(text):\n",
        "    text = re.sub('\"', \"&quot;\",  text)\n",
        "    text = re.sub(\">\", \"&gt;\", text)\n",
        "    text = re.sub(\"<\", \"&lt;\", text)\n",
        "    text = re.sub(\"≤\", \"&le;\", text)\n",
        "    text = re.sub(\"≥\", \"&ge;\", text)\n",
        "    text = re.sub(\"&\", \"&amp;\", text)   \n",
        "    return text\n",
        "#TODO: find examples and apply!!\n",
        "\n",
        "def print_to_file(text):\n",
        "  try:\n",
        "    f = open(CHECKPOINT_PATH + VER + \"_logs.txt\", 'a')\n",
        "  except:\n",
        "    f = open(CHECKPOINT_PATH + VER + \"_logs.txt\", 'w')\n",
        "\n",
        "  print(text)\n",
        "  f.write(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  f.write(\"  -- \")\n",
        "  f.write(text)\n",
        "  f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUXCviVTxAk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "\n",
        "    a = set(str(str1).lower().split()) \n",
        "    b = set(str(str2).lower().split())\n",
        "\n",
        "    c = a.intersection(b)\n",
        "\n",
        "    if (len(a) + len(b) - len(c)) > 0:\n",
        "      return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoajiefWxWQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 120 #96 #TODO: make this value high enough that this doesn't cause an issue in the final setting\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file= ROBERTA_PATH +'vocab-roberta-base.json', \n",
        "    merges_file= ROBERTA_PATH +'merges-roberta-base.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUVVzLs500Xc",
        "colab_type": "text"
      },
      "source": [
        "# READ DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiXCCJOSwnv5",
        "colab_type": "code",
        "outputId": "7f6673ef-f328-4343-dc15-6cba75cecbde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def read_train():\n",
        "    train=pd.read_csv(DATA_PATH + 'train.csv')\n",
        "    train['text']=train['text'].astype(str)\n",
        "    train['selected_text']=train['selected_text'].astype(str)\n",
        "\n",
        "    return train\n",
        "\n",
        "def read_test():\n",
        "    test=pd.read_csv(DATA_PATH + 'test.csv')\n",
        "    test['text']=test['text'].astype(str)\n",
        "\n",
        "    return test\n",
        "\n",
        "if (DEV_MODE == True):\n",
        "  train_df = read_train().sample(frac=.05, random_state=1).reset_index()\n",
        "  test_df = read_test().sample(frac=.05, random_state=1).reset_index()\n",
        "else:\n",
        "  train_df = read_train()\n",
        "  test_df = read_test()\n",
        "\n",
        "\n",
        "print_to_file(\"train_df shape:\" + str(train_df.shape))\n",
        "print_to_file(\"test_df shape:\" + str(test_df.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_df shape:(27481, 4)\n",
            "test_df shape:(3534, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYedSrGP0x3u",
        "colab_type": "text"
      },
      "source": [
        "# PREPROCESS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS9Q043XxfFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = train_df.shape[0]\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(train_df.shape[0]):\n",
        "    \n",
        "    # FIND OVERLAP\n",
        "    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1) \n",
        "        \n",
        "    # ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "    \n",
        "    # START END TOKENS\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
        "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask[k,:len(enc.ids)+5] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+1] = 1\n",
        "        end_tokens[k,toks[-1]+1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c740MhN9xfpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = test_df.shape[0]\n",
        "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(test_df.shape[0]):\n",
        "        \n",
        "    # INPUT_IDS\n",
        "    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
        "    enc = tokenizer.encode(text1)                \n",
        "    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n",
        "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask_t[k,:len(enc.ids)+5] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZK8td4txj1Z",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2eSnUkjxnEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch):\n",
        "    return 3e-5 * 0.2**epoch\n",
        "\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(ROBERTA_PATH + 'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(ROBERTA_PATH + 'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.3)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.3)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49tvlLy6mRsf",
        "colab_type": "code",
        "outputId": "c1f081c4-6793-41ba-fa4b-851c0c8922c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 120)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 120)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 120)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model_1 (TFRobertaMo ((None, 120, 768), ( 124645632   input_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_78 (Dropout)            (None, 120, 768)     0           tf_roberta_model_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "dropout_79 (Dropout)            (None, 120, 768)     0           tf_roberta_model_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_4 (Conv1D)               (None, 120, 128)     196736      dropout_78[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_6 (Conv1D)               (None, 120, 128)     196736      dropout_79[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 120, 128)     0           conv1d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 120, 128)     0           conv1d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_5 (Conv1D)               (None, 120, 64)      16448       leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_7 (Conv1D)               (None, 120, 64)      16448       leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 120, 1)       65          conv1d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 120, 1)       65          conv1d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_2 (Flatten)             (None, 120)          0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "flatten_3 (Flatten)             (None, 120)          0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 120)          0           flatten_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 120)          0           flatten_3[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 125,072,130\n",
            "Trainable params: 125,072,130\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iyatZy1xo2f",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T6hmc6YiywP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.config.experimental_connect_to_host('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.experimental.TPUStrategy(resolver) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jdpn1mOxoHl",
        "colab_type": "code",
        "outputId": "9fd42c91-cd29-4434-873e-e07ff4aa2636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "n_splits = 5\n",
        "\n",
        "jac = []\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n",
        "\n",
        "    print_to_file('#'*25)\n",
        "    print_to_file('### FOLD %i'%(fold+1))\n",
        "    print_to_file('#'*25)\n",
        "\n",
        "\n",
        "    K.clear_session()\n",
        "    if not os.path.exists(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(VER,fold)):\n",
        "        # with strategy.scope():\n",
        "        model = build_model()\n",
        "        model.compile(\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
        "            loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=.25)\n",
        "            )\n",
        "        \n",
        "        reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "        sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "            CHECKPOINT_PATH + '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "            save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "            \n",
        "        X = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
        "        Y = [start_tokens[idxT,], end_tokens[idxT,]]\n",
        "        X_valid = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
        "        Y_valid = [start_tokens[idxV,], end_tokens[idxV,]]\n",
        "        hist = model.fit(\n",
        "            X,\n",
        "            Y,\n",
        "            epochs=3,\n",
        "            batch_size=8,\n",
        "            verbose=DISPLAY,\n",
        "            callbacks=[sv, reduce_lr],\n",
        "            validation_data=(X_valid, Y_valid)\n",
        "            )\n",
        "    else:\n",
        "      print_to_file(\"training Fold %d is skipped!\"%(fold+1))\n",
        "\n",
        "    print_to_file('Loading model...')\n",
        "    model.load_weights(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print_to_file('Predicting OOF...')\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "\n",
        "        if (a>b) and MAX_SUM_PROB: \n",
        "            max_len = len(oof_start[k,])\n",
        "            a = np.tile(oof_start[k,], (max_len, 1))\n",
        "            b = np.tile(oof_end[k,], (max_len, 1))\n",
        "            c = np.tril(a + b.T, k=0).T\n",
        "            c[c == 0] = -1000\n",
        "            a = np.unravel_index(c.argmax(), c.shape)[0]\n",
        "            b = np.unravel_index(c.argmax(), c.shape)[1]\n",
        "\n",
        "        if (a>b):\n",
        "            st = train_df.loc[k,'text'] \n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        \n",
        "        train_df.loc[k,'selected_text_predicted'] = st\n",
        "        train_df.loc[k,'jaccard'] = jaccard(train_df.loc[k,'selected_text_predicted'],train_df.loc[k,'selected_text'])\n",
        "        if train_df.loc[k,'source'] == \"competition\":\n",
        "            all.append(train_df.loc[k,'jaccard'])\n",
        "\n",
        "    jac.append(np.mean(all))\n",
        "    print_to_file('>>>> FOLD %i Jaccard = '%(fold+1) + str(np.mean(all)))\n",
        "    print()\n",
        "\n",
        "\n",
        "print_to_file('Overall averrage of Jaccards = ' + str(np.mean(jac)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            " 977/2748 [=========>....................] - ETA: 3:59 - loss: 1.2630 - activation_loss: 0.6316 - activation_1_loss: 0.6314"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btMpOh5St6by",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #########################\n",
        "# Epoch 1/3\n",
        "# WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
        "# WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
        "# WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
        "# WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
        "# 2748/2748 [==============================] - ETA: 0s - loss: 0.0325 - activation_loss: 0.0165 - activation_1_loss: 0.0160\n",
        "# Epoch 00001: val_loss improved from inf to 0.02835, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v6-roberta-0.h5\n",
        "# 2748/2748 [==============================] - 351s 128ms/step - loss: 0.0325 - activation_loss: 0.0165 - activation_1_loss: 0.0160 - val_loss: 0.0283 - val_activation_loss: 0.0145 - val_activation_1_loss: 0.0138 - lr: 3.0000e-05\n",
        "# Epoch 2/3\n",
        "# 2748/2748 [==============================] - ETA: 0s - loss: 0.0256 - activation_loss: 0.0131 - activation_1_loss: 0.0125\n",
        "# Epoch 00002: val_loss improved from 0.02835 to 0.02735, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v6-roberta-0.h5\n",
        "# 2748/2748 [==============================] - 349s 127ms/step - loss: 0.0256 - activation_loss: 0.0131 - activation_1_loss: 0.0125 - val_loss: 0.0273 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0134 - lr: 6.0000e-06\n",
        "# Epoch 3/3\n",
        "# 2748/2748 [==============================] - ETA: 0s - loss: 0.0240 - activation_loss: 0.0122 - activation_1_loss: 0.0117\n",
        "# Epoch 00003: val_loss did not improve from 0.02735\n",
        "# 2748/2748 [==============================] - 342s 125ms/step - loss: 0.0240 - activation_loss: 0.0122 - activation_1_loss: 0.0117 - val_loss: 0.0275 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0136 - lr: 1.2000e-06\n",
        "# Loading model...\n",
        "# Predicting OOF...\n",
        "# 172/172 [==============================] - 23s 135ms/step\n",
        "# >>>> FOLD 1 Jaccard = 0.7045174031165262\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbA6K8ldxyUc",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pho4ql-hxxYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "DISPLAY=1\n",
        "for i in range(5):\n",
        "    print_to_file('#'*25)\n",
        "    print_to_file('### MODEL %i'%(i+1))\n",
        "    print_to_file('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "    model.load_weights(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(VER,i))\n",
        "\n",
        "    print_to_file('Predicting Test...')\n",
        "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/n_splits\n",
        "    preds_end += preds[1]/n_splits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p31hQI95x4Ra",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RQkmXq4x3Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all = []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "\n",
        "    if (a>b) and MAX_SUM_PROB: \n",
        "        max_len = len(oof_start[k,])\n",
        "        a = np.tile(oof_start[k,], (max_len, 1))\n",
        "        b = np.tile(oof_end[k,], (max_len, 1))\n",
        "        c = np.tril(a + b.T, k=0).T\n",
        "        c[c == 0] = -1000\n",
        "        a = np.unravel_index(c.argmax(), c.shape)[0]\n",
        "        b = np.unravel_index(c.argmax(), c.shape)[1]\n",
        "\n",
        "    if (a>b): \n",
        "        st = test_df.loc[k,'text']\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        st = tokenizer.decode(enc.ids[a-1:b])\n",
        "    all.append(st)\n",
        "\n",
        "test_df['selected_text'] = all\n",
        "test_df['selected_text'] = test_df['selected_text'].apply(add_html)\n",
        "\n",
        "test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
        "test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
        "test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
        "\n",
        "test_df[['textID','selected_text']].to_csv('submission.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}