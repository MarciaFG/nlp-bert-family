{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_roberta_gpu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNMCFH6wCr3XIcIY3WUT/nq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/branjbar/nlp-bert-family/blob/master/tensorflow/tensorflow_roberta_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-2ew8z1lmev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read to use TPU https://blog.tensorflow.org/2019/01/keras-on-tpus-in-colab.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okm4ne1CV7av",
        "colab_type": "code",
        "outputId": "0a4edec7-affd-4600-fdd6-d3b85fc7d1ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To avoid notebook get disconnected\n",
        "\"\"\"\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\"); \n",
        "document.querySelector(\"colab-toolbar-button\").click() \n",
        "}setInterval(ClickConnect,60000)\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "caffeinate\n",
        "\"\"\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncaffeinate\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHM0k54CtHeA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaEKNAAktV7X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjE4irb8tkk0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VmEWu6QwCLX",
        "colab_type": "code",
        "outputId": "8254f9f0-674d-4511-af21-b1bef5516b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Sat May  2 18:43:12 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    42W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGTHRzFTwKSI",
        "colab_type": "code",
        "outputId": "51e686c2-a9c7-49c5-b95e-7e5f8fab1152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!pip install transformers\n",
        "import os\n",
        "import pandas as pd, numpy as np\n",
        "import time, re, math\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "print('TF version',tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "TF version 2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adF-Or6QwwQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KAGGLE_INPUT_PATH = \"../input/\"\n",
        "COLAB_INPUT_PATH = \"/content/drive/My Drive/input/\"\n",
        "INPUT_PATH = COLAB_INPUT_PATH\n",
        "\n",
        "DATA_PATH = INPUT_PATH + \"tweet-sentiment-extraction/\"\n",
        "CHECKPOINT_PATH = INPUT_PATH + \"model_checkpoint/roberta/\"\n",
        "ROBERTA_PATH = INPUT_PATH + 'tf-roberta/'\n",
        "\n",
        "VER='v10'\n",
        "DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
        "\n",
        "DEV_MODE = False # if True only looks at a subset of data\n",
        "MAX_SUM_PROB = True "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2TdPHvh4XdN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_html(text):\n",
        "    text = re.sub(\"&quot;\", '\"', text)\n",
        "    text = re.sub(\"&gt;\", \">\", text)\n",
        "    text = re.sub(\"&lt;\", \"<\", text)\n",
        "    text = re.sub(\"&le;\", \"≤\", text)\n",
        "    text = re.sub(\"&ge;\", \"≥\", text)\n",
        "    text = re.sub(\"&amp;\", \"&\", text)\n",
        "    return text\n",
        "\n",
        "def add_html(text):\n",
        "    text = re.sub('\"', \"&quot;\",  text)\n",
        "    text = re.sub(\">\", \"&gt;\", text)\n",
        "    text = re.sub(\"<\", \"&lt;\", text)\n",
        "    text = re.sub(\"≤\", \"&le;\", text)\n",
        "    text = re.sub(\"≥\", \"&ge;\", text)\n",
        "    text = re.sub(\"&\", \"&amp;\", text)   \n",
        "    return text\n",
        "#TODO: find examples and apply!!\n",
        "\n",
        "def print_to_file(text):\n",
        "  try:\n",
        "    f = open(CHECKPOINT_PATH + VER + \"_logs.txt\", 'a')\n",
        "  except:\n",
        "    f = open(CHECKPOINT_PATH + VER + \"_logs.txt\", 'w')\n",
        "\n",
        "  print(text)\n",
        "  f.write(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  f.write(\"  -- \")\n",
        "  f.write(text)\n",
        "  f.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUXCviVTxAk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "\n",
        "    a = set(str(str1).lower().split()) \n",
        "    b = set(str(str2).lower().split())\n",
        "\n",
        "    c = a.intersection(b)\n",
        "\n",
        "    if (len(a) + len(b) - len(c)) > 0:\n",
        "      return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZoajiefWxWQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LEN = 120 #96 #TODO: make this value high enough that this doesn't cause an issue in the final setting\n",
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file= ROBERTA_PATH +'vocab-roberta-base.json', \n",
        "    merges_file= ROBERTA_PATH +'merges-roberta-base.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUVVzLs500Xc",
        "colab_type": "text"
      },
      "source": [
        "# READ DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiXCCJOSwnv5",
        "colab_type": "code",
        "outputId": "df409d02-05ce-4756-eee0-2a4081cf52db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def read_train():\n",
        "    train=pd.read_csv(DATA_PATH + 'train.csv')\n",
        "    train['text']=train['text'].astype(str)\n",
        "    train['selected_text']=train['selected_text'].astype(str)\n",
        "\n",
        "    return train\n",
        "\n",
        "def read_test():\n",
        "    test=pd.read_csv(DATA_PATH + 'test.csv')\n",
        "    test['text']=test['text'].astype(str)\n",
        "\n",
        "    return test\n",
        "\n",
        "if (DEV_MODE == True):\n",
        "  train_df = read_train().sample(frac=.05, random_state=1).reset_index()\n",
        "  test_df = read_test().sample(frac=.05, random_state=1).reset_index()\n",
        "else:\n",
        "  train_df = read_train()\n",
        "  test_df = read_test()\n",
        "\n",
        "\n",
        "print_to_file(\"train_df shape:\" + str(train_df.shape))\n",
        "print_to_file(\"test_df shape:\" + str(test_df.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_df shape:(27481, 4)\n",
            "test_df shape:(3534, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYedSrGP0x3u",
        "colab_type": "text"
      },
      "source": [
        "# PREPROCESS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GS9Q043XxfFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = train_df.shape[0]\n",
        "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(train_df.shape[0]):\n",
        "    \n",
        "    # FIND OVERLAP\n",
        "    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "    text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
        "    idx = text1.find(text2)\n",
        "    chars = np.zeros((len(text1)))\n",
        "    chars[idx:idx+len(text2)]=1\n",
        "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "    enc = tokenizer.encode(text1) \n",
        "        \n",
        "    # ID_OFFSETS\n",
        "    offsets = []; idx=0\n",
        "    for t in enc.ids:\n",
        "        w = tokenizer.decode([t])\n",
        "        offsets.append((idx,idx+len(w)))\n",
        "        idx += len(w)\n",
        "    \n",
        "    # START END TOKENS\n",
        "    toks = []\n",
        "    for i,(a,b) in enumerate(offsets):\n",
        "        sm = np.sum(chars[a:b])\n",
        "        if sm>0: toks.append(i) \n",
        "        \n",
        "    s_tok = sentiment_id[train_df.loc[k,'sentiment']]\n",
        "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask[k,:len(enc.ids)+5] = 1\n",
        "    if len(toks)>0:\n",
        "        start_tokens[k,toks[0]+1] = 1\n",
        "        end_tokens[k,toks[-1]+1] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c740MhN9xfpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct = test_df.shape[0]\n",
        "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
        "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
        "\n",
        "for k in range(test_df.shape[0]):\n",
        "        \n",
        "    # INPUT_IDS\n",
        "    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
        "    enc = tokenizer.encode(text1)                \n",
        "    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n",
        "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "    attention_mask_t[k,:len(enc.ids)+5] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpronuWYoMbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 8\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f23rq3usmDpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_dataset(idxT, idxV):\n",
        "    \n",
        "    # Trainset\n",
        "    trn_input_ids = input_ids[idxT,]\n",
        "    trn_att_mask = attention_mask[idxT,]\n",
        "    trn_token_type_ids = token_type_ids[idxT,]\n",
        "    \n",
        "    trn_start_tokens = start_tokens[idxT,]\n",
        "    trn_end_tokens = end_tokens[idxT,]\n",
        "    \n",
        "    # Validation set\n",
        "    val_input_ids = input_ids[idxV,]\n",
        "    val_att_mask = attention_mask[idxV,]\n",
        "    val_token_type_ids = token_type_ids[idxV,]\n",
        "    \n",
        "    val_start_tokens = start_tokens[idxV,]\n",
        "    val_end_tokens = end_tokens[idxV,]\n",
        "    \n",
        "    # Generating tf.data object\n",
        "    train_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices(({'input_ids':trn_input_ids, 'attention_mask': trn_att_mask, 'token_type_ids': trn_token_type_ids}, \n",
        "                             {'start_tokens': trn_start_tokens, 'end_tokens': trn_end_tokens}))\n",
        "        .shuffle(2048)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    \n",
        "    valid_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices(({'input_ids':val_input_ids, 'attention_mask': val_att_mask, 'token_type_ids': val_token_type_ids}, \n",
        "                             {'start_tokens': val_start_tokens, 'end_tokens': val_end_tokens}))\n",
        "        .batch(BATCH_SIZE)\n",
        "        .cache()\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    \n",
        "    return trn_input_ids.shape[0]//BATCH_SIZE, train_dataset, valid_dataset\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZK8td4txj1Z",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2eSnUkjxnEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch):\n",
        "    return 3e-5 * 0.2**epoch\n",
        "\n",
        "def build_model():\n",
        "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
        "\n",
        "    config = RobertaConfig.from_pretrained(ROBERTA_PATH + 'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(ROBERTA_PATH + 'pretrained-roberta-base.h5',config=config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(0.3)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(0.3)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49tvlLy6mRsf",
        "colab_type": "code",
        "outputId": "4cd1b161-2e68-484d-dd66-b6ffeae90658",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "model = build_model()\n",
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 120)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 120)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 120)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_roberta_model (TFRobertaMode ((None, 120, 768), ( 124645632   input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_38 (Dropout)            (None, 120, 768)     0           tf_roberta_model[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_39 (Dropout)            (None, 120, 768)     0           tf_roberta_model[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d (Conv1D)                 (None, 120, 128)     196736      dropout_38[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 120, 128)     196736      dropout_39[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu (LeakyReLU)         (None, 120, 128)     0           conv1d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 120, 128)     0           conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 120, 64)      16448       leaky_re_lu[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_3 (Conv1D)               (None, 120, 64)      16448       leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 120, 1)       65          conv1d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 120, 1)       65          conv1d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 120)          0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 120)          0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 120)          0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 120)          0           flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 125,072,130\n",
            "Trainable params: 125,072,130\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iyatZy1xo2f",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T6hmc6YiywP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.config.experimental_connect_to_host('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver('grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "# strategy = tf.distribute.experimental.TPUStrategy(resolver) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Jdpn1mOxoHl",
        "colab_type": "code",
        "outputId": "052b8b70-e1c7-4852-86ce-828cfd17ff85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_splits = 5\n",
        "\n",
        "jac = []\n",
        "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
        "\n",
        "skf = StratifiedKFold(n_splits=n_splits,shuffle=True,random_state=777)\n",
        "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train_df.sentiment.values)):\n",
        "\n",
        "    print_to_file('#'*25)\n",
        "    print_to_file('### FOLD %i'%(fold+1))\n",
        "    print_to_file('#'*25)\n",
        "\n",
        "    # n_steps, trn_dataset, val_dataset = generate_dataset(idxT, idxV)\n",
        "\n",
        "    K.clear_session()\n",
        "    if not os.path.exists(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(VER,fold)):\n",
        "        model = build_model()\n",
        "        model.compile(\n",
        "            optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5),\n",
        "            loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=.25)\n",
        "            )\n",
        "    \n",
        "        reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "        reduce_lr = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "        sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "            CHECKPOINT_PATH + '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "            save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "            \n",
        "        X = [input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]]\n",
        "        Y = [start_tokens[idxT,], end_tokens[idxT,]]\n",
        "        X_valid = [input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]]\n",
        "        Y_valid = [start_tokens[idxV,], end_tokens[idxV,]]\n",
        "        hist = model.fit(\n",
        "            X,\n",
        "            Y,\n",
        "            epochs=3,\n",
        "            batch_size=8,\n",
        "            verbose=DISPLAY,\n",
        "            callbacks=[sv, reduce_lr],\n",
        "            validation_data=(X_valid, Y_valid)\n",
        "            )\n",
        "    else:\n",
        "      print_to_file(\"training Fold %d is skipped!\"%(fold+1))\n",
        "\n",
        "    print_to_file('Loading model...')\n",
        "    model.load_weights(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(VER,fold))\n",
        "    \n",
        "    print_to_file('Predicting OOF...')\n",
        "    # oof_start[idxV,],oof_end[idxV,] = model.predict(val_dataset,verbose=DISPLAY)\n",
        "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
        "    \n",
        "    # DISPLAY FOLD JACCARD\n",
        "    all = []\n",
        "    for k in idxV:\n",
        "        a = np.argmax(oof_start[k,])\n",
        "        b = np.argmax(oof_end[k,])\n",
        "\n",
        "        if (a>b) and MAX_SUM_PROB: \n",
        "            max_len = len(oof_start[k,])\n",
        "            a = np.tile(oof_start[k,], (max_len, 1))\n",
        "            b = np.tile(oof_end[k,], (max_len, 1))\n",
        "            c = np.tril(a + b.T, k=0).T\n",
        "            c[c == 0] = -1000\n",
        "            a = np.unravel_index(c.argmax(), c.shape)[0]\n",
        "            b = np.unravel_index(c.argmax(), c.shape)[1]\n",
        "\n",
        "        if (a>b):\n",
        "            st = train_df.loc[k,'text'] \n",
        "        else:\n",
        "            text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "            enc = tokenizer.encode(text1)\n",
        "            st = tokenizer.decode(enc.ids[a-1:b])\n",
        "        \n",
        "        train_df.loc[k,'selected_text_predicted'] = st\n",
        "        train_df.loc[k,'jaccard'] = jaccard(train_df.loc[k,'selected_text_predicted'],train_df.loc[k,'selected_text'])\n",
        "        all.append(train_df.loc[k,'jaccard'])\n",
        "\n",
        "    jac.append(np.mean(all))\n",
        "    print_to_file('>>>> FOLD %i Jaccard = '%(fold+1) + str(np.mean(all)))\n",
        "    print()\n",
        "\n",
        "\n",
        "print_to_file('Overall averrage of Jaccards = ' + str(np.mean(jac)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2748/2748 [==============================] - ETA: 0s - loss: 1.2617 - activation_loss: 0.6309 - activation_1_loss: 0.6308\n",
            "Epoch 00001: val_loss improved from inf to 1.26031, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-0.h5\n",
            "2748/2748 [==============================] - 418s 152ms/step - loss: 1.2617 - activation_loss: 0.6309 - activation_1_loss: 0.6308 - val_loss: 1.2603 - val_activation_loss: 0.6302 - val_activation_1_loss: 0.6301 - lr: 3.0000e-05\n",
            "Epoch 2/3\n",
            "2748/2748 [==============================] - ETA: 0s - loss: 1.2598 - activation_loss: 0.6300 - activation_1_loss: 0.6298\n",
            "Epoch 00002: val_loss improved from 1.26031 to 1.26003, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-0.h5\n",
            "2748/2748 [==============================] - 417s 152ms/step - loss: 1.2598 - activation_loss: 0.6300 - activation_1_loss: 0.6298 - val_loss: 1.2600 - val_activation_loss: 0.6301 - val_activation_1_loss: 0.6299 - lr: 6.0000e-06\n",
            "Epoch 3/3\n",
            "2748/2748 [==============================] - ETA: 0s - loss: 1.2595 - activation_loss: 0.6298 - activation_1_loss: 0.6297\n",
            "Epoch 00003: val_loss improved from 1.26003 to 1.26002, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-0.h5\n",
            "2748/2748 [==============================] - 418s 152ms/step - loss: 1.2595 - activation_loss: 0.6298 - activation_1_loss: 0.6297 - val_loss: 1.2600 - val_activation_loss: 0.6301 - val_activation_1_loss: 0.6299 - lr: 1.2000e-06\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 29s 166ms/step\n",
            ">>>> FOLD 1 Jaccard = 0.7065603330492105\n",
            "\n",
            "#########################\n",
            "### FOLD 2\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2617 - activation_loss: 0.6309 - activation_1_loss: 0.6308\n",
            "Epoch 00001: val_loss improved from inf to 1.26030, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-1.h5\n",
            "2749/2749 [==============================] - 464s 169ms/step - loss: 1.2617 - activation_loss: 0.6309 - activation_1_loss: 0.6308 - val_loss: 1.2603 - val_activation_loss: 0.6302 - val_activation_1_loss: 0.6301 - lr: 3.0000e-05\n",
            "Epoch 2/3\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2599 - activation_loss: 0.6300 - activation_1_loss: 0.6299\n",
            "Epoch 00002: val_loss improved from 1.26030 to 1.25989, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-1.h5\n",
            "2749/2749 [==============================] - 464s 169ms/step - loss: 1.2599 - activation_loss: 0.6300 - activation_1_loss: 0.6299 - val_loss: 1.2599 - val_activation_loss: 0.6300 - val_activation_1_loss: 0.6299 - lr: 6.0000e-06\n",
            "Epoch 3/3\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2595 - activation_loss: 0.6298 - activation_1_loss: 0.6297\n",
            "Epoch 00003: val_loss did not improve from 1.25989\n",
            "2749/2749 [==============================] - 460s 167ms/step - loss: 1.2595 - activation_loss: 0.6298 - activation_1_loss: 0.6297 - val_loss: 1.2599 - val_activation_loss: 0.6300 - val_activation_1_loss: 0.6299 - lr: 1.2000e-06\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 28s 165ms/step\n",
            ">>>> FOLD 2 Jaccard = 0.7095336167280826\n",
            "\n",
            "#########################\n",
            "### FOLD 3\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2615 - activation_loss: 0.6308 - activation_1_loss: 0.6307\n",
            "Epoch 00001: val_loss improved from inf to 1.26022, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-2.h5\n",
            "2749/2749 [==============================] - 466s 169ms/step - loss: 1.2615 - activation_loss: 0.6308 - activation_1_loss: 0.6307 - val_loss: 1.2602 - val_activation_loss: 0.6301 - val_activation_1_loss: 0.6301 - lr: 3.0000e-05\n",
            "Epoch 2/3\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2599 - activation_loss: 0.6300 - activation_1_loss: 0.6299\n",
            "Epoch 00002: val_loss improved from 1.26022 to 1.25993, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-2.h5\n",
            "2749/2749 [==============================] - 466s 170ms/step - loss: 1.2599 - activation_loss: 0.6300 - activation_1_loss: 0.6299 - val_loss: 1.2599 - val_activation_loss: 0.6300 - val_activation_1_loss: 0.6299 - lr: 6.0000e-06\n",
            "Epoch 3/3\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2595 - activation_loss: 0.6298 - activation_1_loss: 0.6297\n",
            "Epoch 00003: val_loss did not improve from 1.25993\n",
            "2749/2749 [==============================] - 462s 168ms/step - loss: 1.2595 - activation_loss: 0.6298 - activation_1_loss: 0.6297 - val_loss: 1.2600 - val_activation_loss: 0.6300 - val_activation_1_loss: 0.6299 - lr: 1.2000e-06\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 28s 165ms/step\n",
            ">>>> FOLD 3 Jaccard = 0.7066203819302925\n",
            "\n",
            "#########################\n",
            "### FOLD 4\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2615 - activation_loss: 0.6308 - activation_1_loss: 0.6307\n",
            "Epoch 00001: val_loss improved from inf to 1.26056, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-3.h5\n",
            "2749/2749 [==============================] - 466s 169ms/step - loss: 1.2615 - activation_loss: 0.6308 - activation_1_loss: 0.6307 - val_loss: 1.2606 - val_activation_loss: 0.6303 - val_activation_1_loss: 0.6302 - lr: 3.0000e-05\n",
            "Epoch 2/3\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2598 - activation_loss: 0.6300 - activation_1_loss: 0.6298\n",
            "Epoch 00002: val_loss improved from 1.26056 to 1.25988, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-3.h5\n",
            "2749/2749 [==============================] - 468s 170ms/step - loss: 1.2598 - activation_loss: 0.6300 - activation_1_loss: 0.6298 - val_loss: 1.2599 - val_activation_loss: 0.6300 - val_activation_1_loss: 0.6299 - lr: 6.0000e-06\n",
            "Epoch 3/3\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2595 - activation_loss: 0.6298 - activation_1_loss: 0.6297\n",
            "Epoch 00003: val_loss did not improve from 1.25988\n",
            "2749/2749 [==============================] - 462s 168ms/step - loss: 1.2595 - activation_loss: 0.6298 - activation_1_loss: 0.6297 - val_loss: 1.2599 - val_activation_loss: 0.6300 - val_activation_1_loss: 0.6299 - lr: 1.2000e-06\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 28s 164ms/step\n",
            ">>>> FOLD 4 Jaccard = 0.7134166654412399\n",
            "\n",
            "#########################\n",
            "### FOLD 5\n",
            "#########################\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2620 - activation_loss: 0.6309 - activation_1_loss: 0.6311\n",
            "Epoch 00001: val_loss improved from inf to 1.25999, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-4.h5\n",
            "2749/2749 [==============================] - 465s 169ms/step - loss: 1.2620 - activation_loss: 0.6309 - activation_1_loss: 0.6311 - val_loss: 1.2600 - val_activation_loss: 0.6301 - val_activation_1_loss: 0.6299 - lr: 3.0000e-05\n",
            "Epoch 2/3\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2599 - activation_loss: 0.6300 - activation_1_loss: 0.6299\n",
            "Epoch 00002: val_loss improved from 1.25999 to 1.25990, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v10-roberta-4.h5\n",
            "2749/2749 [==============================] - 465s 169ms/step - loss: 1.2599 - activation_loss: 0.6300 - activation_1_loss: 0.6299 - val_loss: 1.2599 - val_activation_loss: 0.6301 - val_activation_1_loss: 0.6298 - lr: 6.0000e-06\n",
            "Epoch 3/3\n",
            "2749/2749 [==============================] - ETA: 0s - loss: 1.2596 - activation_loss: 0.6298 - activation_1_loss: 0.6297\n",
            "Epoch 00003: val_loss did not improve from 1.25990\n",
            "2749/2749 [==============================] - 461s 168ms/step - loss: 1.2596 - activation_loss: 0.6298 - activation_1_loss: 0.6297 - val_loss: 1.2599 - val_activation_loss: 0.6301 - val_activation_1_loss: 0.6299 - lr: 1.2000e-06\n",
            "Loading model...\n",
            "Predicting OOF...\n",
            "172/172 [==============================] - 28s 165ms/step\n",
            ">>>> FOLD 5 Jaccard = 0.7151962356530397\n",
            "\n",
            "Overall averrage of Jaccards = 0.710265446560373\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMCadZ05G-sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btMpOh5St6by",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #########################\n",
        "# Epoch 1/3\n",
        "# WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
        "# WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
        "# WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
        "# WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
        "# 2748/2748 [==============================] - ETA: 0s - loss: 0.0325 - activation_loss: 0.0165 - activation_1_loss: 0.0160\n",
        "# Epoch 00001: val_loss improved from inf to 0.02835, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v6-roberta-0.h5\n",
        "# 2748/2748 [==============================] - 351s 128ms/step - loss: 0.0325 - activation_loss: 0.0165 - activation_1_loss: 0.0160 - val_loss: 0.0283 - val_activation_loss: 0.0145 - val_activation_1_loss: 0.0138 - lr: 3.0000e-05\n",
        "# Epoch 2/3\n",
        "# 2748/2748 [==============================] - ETA: 0s - loss: 0.0256 - activation_loss: 0.0131 - activation_1_loss: 0.0125\n",
        "# Epoch 00002: val_loss improved from 0.02835 to 0.02735, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/v6-roberta-0.h5\n",
        "# 2748/2748 [==============================] - 349s 127ms/step - loss: 0.0256 - activation_loss: 0.0131 - activation_1_loss: 0.0125 - val_loss: 0.0273 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0134 - lr: 6.0000e-06\n",
        "# Epoch 3/3\n",
        "# 2748/2748 [==============================] - ETA: 0s - loss: 0.0240 - activation_loss: 0.0122 - activation_1_loss: 0.0117\n",
        "# Epoch 00003: val_loss did not improve from 0.02735\n",
        "# 2748/2748 [==============================] - 342s 125ms/step - loss: 0.0240 - activation_loss: 0.0122 - activation_1_loss: 0.0117 - val_loss: 0.0275 - val_activation_loss: 0.0140 - val_activation_1_loss: 0.0136 - lr: 1.2000e-06\n",
        "# Loading model...\n",
        "# Predicting OOF...\n",
        "# 172/172 [==============================] - 23s 135ms/step\n",
        "# >>>> FOLD 1 Jaccard = 0.7045174031165262\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQdIoWfnIWZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbA6K8ldxyUc",
        "colab_type": "text"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pho4ql-hxxYC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "6955d202-3eb8-4a7b-a348-408711ef07f0"
      },
      "source": [
        "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
        "DISPLAY=1\n",
        "for i in range(5):\n",
        "    print_to_file('#'*25)\n",
        "    print_to_file('### MODEL %i'%(i+1))\n",
        "    print_to_file('#'*25)\n",
        "    \n",
        "    K.clear_session()\n",
        "    model = build_model()\n",
        "    model.load_weights(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(VER,i))\n",
        "\n",
        "    print_to_file('Predicting Test...')\n",
        "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
        "    preds_start += preds[0]/n_splits\n",
        "    preds_end += preds[1]/n_splits\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#########################\n",
            "### MODEL 1\n",
            "#########################\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 19s 167ms/step\n",
            "#########################\n",
            "### MODEL 2\n",
            "#########################\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 18s 164ms/step\n",
            "#########################\n",
            "### MODEL 3\n",
            "#########################\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 18s 163ms/step\n",
            "#########################\n",
            "### MODEL 4\n",
            "#########################\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 18s 165ms/step\n",
            "#########################\n",
            "### MODEL 5\n",
            "#########################\n",
            "Predicting Test...\n",
            "111/111 [==============================] - 18s 163ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sr1WQlLIlO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QAU9f5qIzsW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p31hQI95x4Ra",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RQkmXq4x3Wv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all = []\n",
        "for k in range(input_ids_t.shape[0]):\n",
        "    a = np.argmax(preds_start[k,])\n",
        "    b = np.argmax(preds_end[k,])\n",
        "\n",
        "    if (a>b) and MAX_SUM_PROB: \n",
        "        max_len = len(oof_start[k,])\n",
        "        a = np.tile(oof_start[k,], (max_len, 1))\n",
        "        b = np.tile(oof_end[k,], (max_len, 1))\n",
        "        c = np.tril(a + b.T, k=0).T\n",
        "        c[c == 0] = -1000\n",
        "        a = np.unravel_index(c.argmax(), c.shape)[0]\n",
        "        b = np.unravel_index(c.argmax(), c.shape)[1]\n",
        "\n",
        "    if (a>b): \n",
        "        st = test_df.loc[k,'text']\n",
        "    else:\n",
        "        text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n",
        "        enc = tokenizer.encode(text1)\n",
        "        st = tokenizer.decode(enc.ids[a-1:b])\n",
        "    all.append(st)\n",
        "\n",
        "test_df['selected_text'] = all\n",
        "test_df['selected_text'] = test_df['selected_text'].apply(add_html)\n",
        "\n",
        "test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('!!!!', '!') if len(x.split())==1 else x)\n",
        "test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('..', '.') if len(x.split())==1 else x)\n",
        "test_df['selected_text'] = test_df['selected_text'].apply(lambda x: x.replace('...', '.') if len(x.split())==1 else x)\n",
        "\n",
        "test_df[['textID','selected_text']].to_csv('submission.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}