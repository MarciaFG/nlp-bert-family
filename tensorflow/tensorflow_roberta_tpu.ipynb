{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_roberta_tpu.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNo2GDWT1mbbiB4KWrac7a7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/branjbar/nlp-bert-family/blob/master/tensorflow/tensorflow_roberta_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y31um2QhaaUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/rftexas/roberta-with-tensorflow-on-tpu/comments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZGGy9xat9YB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d9fb15d0-e97b-47e8-e8ad-03cf88382602"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSQuuYy7t95n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "78c4c3c8-d9d2-4fec-d110-2fd34a47f71a"
      },
      "source": [
        "!pip install transformers\n",
        "import os\n",
        "import pandas as pd, numpy as np\n",
        "import time, re, math\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "print('TF version',tf.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "TF version 2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U70KuuAbuIYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "e2e0ab6e-f944-477e-e653-142c8accbc51"
      },
      "source": [
        "# Detect hardware\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError: # If TPU not found\n",
        "  tpu = None\n",
        "\n",
        "# Select appropriate distribution strategy\n",
        "if tpu:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    # This is the TPU initialization code that has to be at the beginning.\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # Default strategy that works on CPU and single GPU\n",
        "    print('Running on CPU instead')\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.51.87.242:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.51.87.242:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  ['10.51.87.242:8470']\n",
            "Number of accelerators:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D3cD48LelY0",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWy8WQqtuBE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KAGGLE_INPUT_PATH = \"../input/\"\n",
        "COLAB_INPUT_PATH = \"/content/drive/My Drive/input/\"\n",
        "INPUT_PATH = COLAB_INPUT_PATH\n",
        "\n",
        "DATA_PATH = INPUT_PATH + \"tweet-sentiment-extraction/\"\n",
        "CHECKPOINT_PATH = INPUT_PATH + \"model_checkpoint/roberta/\"\n",
        "ROBERTA_PATH = INPUT_PATH + 'tf-roberta/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPa9W-ocewcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file= ROBERTA_PATH +'vocab-roberta-base.json', \n",
        "    merges_file= ROBERTA_PATH +'merges-roberta-base.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "SENTIMENT_ID = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpYqp0mIeo4s",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iTERuGfe6fJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "b9d1af35-affc-4bc3-e508-3127ed04e5ac"
      },
      "source": [
        "class Config():\n",
        "    def __init__(self, config_path=None):\n",
        "\n",
        "        if config_path:\n",
        "            with open(config_path,'r') as json_file:\n",
        "                self.__dict__ = json.load(json_file)\n",
        "        else:\n",
        "            \n",
        "            self.version='tf-tpu-v1'\n",
        "\n",
        "            # preprocessing\n",
        "            self.max_len = 120\n",
        "            # self.tweet_size_to_ignore = 1\n",
        "\n",
        "            # learning\n",
        "            self.drop_out = 0.3\n",
        "            self.conv1d_size_1 = 128\n",
        "            self.conv1d_size_2 = 64\n",
        "            self.lr = 3e-5\n",
        "            self.lr_decrease = 0.2\n",
        "            self.label_smoothing = 0.25\n",
        "            self.train_batch_size = 8\n",
        "            self.valid_batch_size = 16\n",
        "            self.num_epochs = 3\n",
        "            self.num_folds = 5\n",
        "            self.smart_span = True\n",
        "            # dev\n",
        "            self.num_folds_to_train = 1\n",
        "            self.dev = True\n",
        "            self.overwrite_model = True\n",
        "        \n",
        "        print(str(self.__dict__))\n",
        "\n",
        "    def get(self, var):\n",
        "        return getattr(self, var, None)\n",
        "\n",
        "\n",
        "# config_file_name = \"config-json-tpu-v28.txt\"\n",
        "# config_file_path = os.path.join(CHECKPOINT_PATH_HP_TUNING,config_file_name)\n",
        "# if os.path.exists(config_file_path):\n",
        "    # config = Config(config_file_path)\n",
        "# else:\n",
        "    # print(\"config file not found! Default configuration will be used\")\n",
        "config = Config()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'version': 'tf-tpu-v1', 'max_len': 120, 'drop_out': 0.3, 'conv1d_size_1': 128, 'conv1d_size_2': 64, 'lr': 3e-05, 'lr_decrease': 0.2, 'label_smoothing': 0.25, 'train_batch_size': 8, 'valid_batch_size': 16, 'num_epochs': 3, 'num_folds': 5, 'smart_span': True, 'num_folds_to_train': 1, 'dev': True, 'overwrite_model': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6neYZwqEeq9T",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ks9Sj0CuHWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_to_file(config, text):\n",
        "  try:\n",
        "    f = open(CHECKPOINT_PATH + config.version + \"_logs.txt\", 'a')\n",
        "  except:\n",
        "    f = open(CHECKPOINT_PATH + config.version + \"_logs.txt\", 'w')\n",
        "\n",
        "  print(text)\n",
        "  f.write(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  f.write(\"  -- \")\n",
        "  f.write(text)\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "\n",
        "    a = set(str(str1).lower().split()) \n",
        "    b = set(str(str2).lower().split())\n",
        "\n",
        "    c = a.intersection(b)\n",
        "\n",
        "    if (len(a) + len(b) - len(c)) > 0:\n",
        "      return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIgKJIhegxRV",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaj2JglXuVhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def import_data(config):\n",
        "    def read_train():\n",
        "        train=pd.read_csv(DATA_PATH + 'train.csv')\n",
        "        train['text']=train['text'].astype(str)\n",
        "        train['selected_text']=train['selected_text'].astype(str)\n",
        "\n",
        "        return train\n",
        "\n",
        "    def read_test():\n",
        "        test=pd.read_csv(DATA_PATH + 'test.csv')\n",
        "        test['text']=test['text'].astype(str)\n",
        "\n",
        "        return test\n",
        "\n",
        "    if (config.dev == True):\n",
        "        train_df = read_train().sample(frac=.05, random_state=1).reset_index()\n",
        "        test_df = read_test().sample(frac=.05, random_state=1).reset_index()\n",
        "    else:\n",
        "        train_df = read_train()\n",
        "        test_df = read_test()\n",
        "\n",
        "\n",
        "    print_to_file(config, \"train_df shape:\" + str(train_df.shape))\n",
        "    print_to_file(config, \"test_df shape:\" + str(test_df.shape))\n",
        "    \n",
        "    return train_df, test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOZTcNnZhICe",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOXSqpFxuXGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_train(config, train_df):\n",
        "    ct = train_df.shape[0]\n",
        "    input_ids = np.ones((ct,config.max_len),dtype='int32')\n",
        "    attention_mask = np.zeros((ct,config.max_len),dtype='int32')\n",
        "    token_type_ids = np.zeros((ct,config.max_len),dtype='int32')\n",
        "    start_tokens = np.zeros((ct,config.max_len),dtype='int32')\n",
        "    end_tokens = np.zeros((ct,config.max_len),dtype='int32')\n",
        "\n",
        "    for k in range(train_df.shape[0]):\n",
        "        \n",
        "        # FIND OVERLAP\n",
        "        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "        text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
        "        idx = text1.find(text2)\n",
        "        chars = np.zeros((len(text1)))\n",
        "        chars[idx:idx+len(text2)]=1\n",
        "        if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "        enc = tokenizer.encode(text1) \n",
        "            \n",
        "        # ID_OFFSETS\n",
        "        offsets = []; idx=0\n",
        "        for t in enc.ids:\n",
        "            w = tokenizer.decode([t])\n",
        "            offsets.append((idx,idx+len(w)))\n",
        "            idx += len(w)\n",
        "        \n",
        "        # START END TOKENS\n",
        "        toks = []\n",
        "        for i,(a,b) in enumerate(offsets):\n",
        "            sm = np.sum(chars[a:b])\n",
        "            if sm>0: toks.append(i) \n",
        "            \n",
        "        s_tok = SENTIMENT_ID[train_df.loc[k,'sentiment']]\n",
        "        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "        attention_mask[k,:len(enc.ids)+5] = 1\n",
        "        if len(toks)>0:\n",
        "            start_tokens[k,toks[0]+1] = 1\n",
        "            end_tokens[k,toks[-1]+1] = 1\n",
        "    return  input_ids, attention_mask, token_type_ids, start_tokens, end_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA_vXWpuucFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_dataset(config, idxT, idxV, input_ids, attention_mask, token_type_ids, start_tokens, end_tokens):\n",
        "    \n",
        "    # Trainset\n",
        "    trn_input_ids = input_ids[idxT,]\n",
        "    trn_att_mask = attention_mask[idxT,]\n",
        "    trn_token_type_ids = token_type_ids[idxT,]\n",
        "    \n",
        "    trn_start_tokens = start_tokens[idxT,]\n",
        "    trn_end_tokens = end_tokens[idxT,]\n",
        "    \n",
        "    # Validation set\n",
        "    val_input_ids = input_ids[idxV,]\n",
        "    val_att_mask = attention_mask[idxV,]\n",
        "    val_token_type_ids = token_type_ids[idxV,]\n",
        "    \n",
        "    val_start_tokens = start_tokens[idxV,]\n",
        "    val_end_tokens = end_tokens[idxV,]\n",
        "    \n",
        "    # Generating tf.data object\n",
        "    train_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices(({'input_ids':trn_input_ids, 'attention_mask': trn_att_mask, 'token_type_ids': trn_token_type_ids}, \n",
        "                             {'start_tokens': trn_start_tokens, 'end_tokens': trn_end_tokens}))\n",
        "        .shuffle(2048)\n",
        "        .batch(config.train_batch_size)\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    \n",
        "    valid_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices(({'input_ids':val_input_ids, 'attention_mask': val_att_mask, 'token_type_ids': val_token_type_ids}, \n",
        "                             {'start_tokens': val_start_tokens, 'end_tokens': val_end_tokens}))\n",
        "        .batch(config.valid_batch_size)\n",
        "        .cache()\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvUiECrcjbh6",
        "colab_type": "text"
      },
      "source": [
        "# Design Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t37zAgJTueNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch):\n",
        "    return config.lr * config.lr_decrease**epoch\n",
        "\n",
        "def build_model(config):\n",
        "    ids = tf.keras.layers.Input((config.max_len,), dtype=tf.int32, name='input_ids')\n",
        "    att = tf.keras.layers.Input((config.max_len,), dtype=tf.int32, name='attention_mask')\n",
        "    tok = tf.keras.layers.Input((config.max_len,), dtype=tf.int32, name='token_type_ids')\n",
        "\n",
        "    robert_config = RobertaConfig.from_pretrained(ROBERTA_PATH + 'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(ROBERTA_PATH + 'pretrained-roberta-base.h5',config=robert_config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(config.drop_out)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(config.conv1d_size_1, 2, padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(config.conv1d_size_2, 2, padding='same')(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax', name='start_tokens')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(config.drop_out)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(config.conv1d_size_1, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(config.conv1d_size_2, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax', name='end_tokens')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ex5nAdzjdtb",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KlUcgkbjgcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_train(config, train_df, input_ids, attention_mask, token_type_ids, start_tokens, end_tokens):\n",
        "    jac = []\n",
        "    oof_start = np.zeros((input_ids.shape[0],config.max_len))\n",
        "    oof_end = np.zeros((input_ids.shape[0],config.max_len))\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=config.num_folds, shuffle=True, random_state=777)\n",
        "    for fold, (idxT,idxV) in enumerate(skf.split(input_ids, train_df.sentiment.values)):\n",
        "        if fold < config.num_folds_to_train:\n",
        "            print_to_file(config, '#'*25)\n",
        "            print_to_file(config, '### FOLD %i'%(fold+1))\n",
        "            print_to_file(config, '#'*25)\n",
        "\n",
        "            trn_dataset, val_dataset = generate_dataset(config, idxT, idxV, input_ids, attention_mask, token_type_ids, start_tokens, end_tokens)\n",
        "\n",
        "            if config.overwrite_model or (not os.path.exists(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(config.version,fold))):\n",
        "                K.clear_session()\n",
        "                with strategy.scope():\n",
        "                    model = build_model(config)\n",
        "                    model.compile(\n",
        "                        optimizer = tf.keras.optimizers.Adam(learning_rate=config.lr),\n",
        "                        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=config.label_smoothing)\n",
        "                        )\n",
        "            \n",
        "                reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: config.lr * config.lr_decrease**epoch, verbose=1)\n",
        "\n",
        "                sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "                    CHECKPOINT_PATH + '%s-roberta-%i.h5'%(config.version,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "                    save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "                hist = model.fit(\n",
        "                    trn_dataset,\n",
        "                    epochs=config.num_epochs,\n",
        "                    batch_size=config.train_batch_size,\n",
        "                    verbose=1,\n",
        "                    callbacks=[sv, reduce_lr],\n",
        "                    validation_data=val_dataset\n",
        "                    )\n",
        "            else:\n",
        "                print_to_file(config, \"training Fold %d is skipped!\"%(fold+1))\n",
        "\n",
        "            print_to_file(config, 'Loading model...')\n",
        "            model.load_weights(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(config.version,fold))\n",
        "            \n",
        "            print_to_file(config, 'Predicting OOF...')\n",
        "            oof_start[idxV,],oof_end[idxV,] = model.predict(val_dataset, verbose=1)\n",
        "            \n",
        "            # DISPLAY FOLD JACCARD\n",
        "            all = []\n",
        "            for k in idxV:\n",
        "                a = np.argmax(oof_start[k,])\n",
        "                b = np.argmax(oof_end[k,])\n",
        "\n",
        "                if (a>b) and config.smart_span: \n",
        "                    max_len = len(oof_start[k,])\n",
        "                    a = np.tile(oof_start[k,], (max_len, 1))\n",
        "                    b = np.tile(oof_end[k,], (max_len, 1))\n",
        "                    c = np.tril(a + b.T, k=0).T\n",
        "                    c[c == 0] = -1000\n",
        "                    a = np.unravel_index(c.argmax(), c.shape)[0]\n",
        "                    b = np.unravel_index(c.argmax(), c.shape)[1]\n",
        "\n",
        "                if (a>b):\n",
        "                    st = train_df.loc[k,'text'] \n",
        "                else:\n",
        "                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "                    enc = tokenizer.encode(text1)\n",
        "                    st = tokenizer.decode(enc.ids[a-1:b])\n",
        "                \n",
        "                train_df.loc[k,'selected_text_predicted'] = st\n",
        "                train_df.loc[k,'jaccard'] = jaccard(train_df.loc[k,'selected_text_predicted'],train_df.loc[k,'selected_text'])\n",
        "                all.append(train_df.loc[k,'jaccard'])\n",
        "\n",
        "            jac.append(np.mean(all))\n",
        "            print_to_file(config, '>>>> FOLD %i Jaccard = '%(fold+1) + str(np.mean(all)))\n",
        "\n",
        "\n",
        "    print_to_file(config, 'Overall averrage of Jaccards in %i folds = '% (fold+1) + str(np.mean(jac)))\n",
        "    return (np.mean(jac))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0q0souUmkJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "9f613bc8-cfd5-430b-ec44-d1b9d1a34d16"
      },
      "source": [
        "train_df, test_df = import_data(config)\n",
        "input_ids, attention_mask, token_type_ids, start_tokens, end_tokens = preprocess_train(config, train_df)\n",
        "run_train(config, train_df, input_ids, attention_mask, token_type_ids, start_tokens, end_tokens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_df shape:(1374, 5)\n",
            "test_df shape:(177, 4)\n",
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 3e-05.\n",
            "Epoch 1/3\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "138/138 [==============================] - ETA: 0s - loss: 1.2614 - end_tokens_loss: 0.6307 - start_tokens_loss: 0.6307\n",
            "Epoch 00001: val_loss improved from inf to 1.21963, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/tf-tpu-v1-roberta-0.h5\n",
            "138/138 [==============================] - 27s 199ms/step - loss: 1.2614 - end_tokens_loss: 0.6307 - start_tokens_loss: 0.6307 - val_loss: 1.2196 - val_end_tokens_loss: 0.6096 - val_start_tokens_loss: 0.6101 - lr: 3.0000e-05\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 6e-06.\n",
            "Epoch 2/3\n",
            "138/138 [==============================] - ETA: 0s - loss: 1.2575 - end_tokens_loss: 0.6287 - start_tokens_loss: 0.6288\n",
            "Epoch 00002: val_loss improved from 1.21963 to 1.21935, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/tf-tpu-v1-roberta-0.h5\n",
            "138/138 [==============================] - 27s 198ms/step - loss: 1.2575 - end_tokens_loss: 0.6287 - start_tokens_loss: 0.6288 - val_loss: 1.2193 - val_end_tokens_loss: 0.6096 - val_start_tokens_loss: 0.6098 - lr: 6.0000e-06\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 1.2000000000000004e-06.\n",
            "Epoch 3/3\n",
            " 29/138 [=====>........................] - ETA: 9s - loss: 1.2627 - end_tokens_loss: 0.6315 - start_tokens_loss: 0.6311"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dev0RlUWf4PY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HyperParameter Tunning\n",
        "session_num = 0\n",
        "for num_units1 in HP_NUM_UNITS1.domain.values:\n",
        "  for num_units2 in HP_NUM_UNITS2.domain.values:\n",
        "    for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
        "      for l2 in (HP_L2.domain.min_value, HP_L2.domain.max_value):\n",
        "        for optimizer in HP_OPTIMIZER.domain.values:\n",
        "          \n",
        "          hparams = {\n",
        "              HP_NUM_UNITS1: num_units1,\n",
        "              HP_NUM_UNITS2: num_units2,\n",
        "              HP_DROPOUT: dropout_rate,\n",
        "              HP_L2: l2,\n",
        "              HP_OPTIMIZER: optimizer\n",
        "              \n",
        "          }\n",
        "          run_name = \"run-%d\" % session_num\n",
        "          print('--- Starting trial: %s' % run_name)\n",
        "          print({h.name: hparams[h] for h in hparams})\n",
        "          run('logs/hparam_tuning/' + run_name, hparams)\n",
        "          session_num += 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}