{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_roberta_tpu.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOuLoRkIltEGPIORzn2ZPj9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/branjbar/nlp-bert-family/blob/master/tensorflow/tensorflow_roberta_tpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y31um2QhaaUJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://www.kaggle.com/rftexas/roberta-with-tensorflow-on-tpu/comments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZGGy9xat9YB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d0fbd912-1c48-4707-9b2e-a393f9dac545"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSlyvUeAVG4W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!export TF_CUDNN_WORKSPACE_LIMIT_IN_MB=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSQuuYy7t95n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "7a06bca7-6d53-4229-e98c-4f51483d0a18"
      },
      "source": [
        "!pip install transformers\n",
        "import os\n",
        "import pandas as pd, numpy as np\n",
        "import time, re, math\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from transformers import *\n",
        "import tokenizers\n",
        "import random\n",
        "print('TF version',tf.__version__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 3.3MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 16.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 19.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 48.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3c8455d2409d7050fd4ec87db64ba2e89c3dcc56d9456fbdb76ee05e93468691\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.5.2 transformers-2.8.0\n",
            "TF version 2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U70KuuAbuIYV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "outputId": "9ff4ed6f-ee6b-4b98-a943-a35c6dbce28f"
      },
      "source": [
        "# Detect hardware\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "except ValueError: # If TPU not found\n",
        "  tpu = None\n",
        "\n",
        "# Select appropriate distribution strategy\n",
        "if tpu:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "    # This is the TPU initialization code that has to be at the beginning.\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
        "\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \n",
        "else:\n",
        "    strategy = tf.distribute.get_strategy() # Default strategy that works on CPU and single GPU\n",
        "    print('Running on CPU instead')\n",
        "print(\"Number of accelerators: \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.14.190.138:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.14.190.138:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running on TPU  ['10.14.190.138:8470']\n",
            "Number of accelerators:  8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D3cD48LelY0",
        "colab_type": "text"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWy8WQqtuBE8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KAGGLE_INPUT_PATH = \"../input/\"\n",
        "COLAB_INPUT_PATH = \"/content/drive/My Drive/input/\"\n",
        "INPUT_PATH = COLAB_INPUT_PATH\n",
        "\n",
        "DATA_PATH = INPUT_PATH + \"tweet-sentiment-extraction/\"\n",
        "CHECKPOINT_PATH = INPUT_PATH + \"model_checkpoint/roberta/\"\n",
        "ROBERTA_PATH = INPUT_PATH + 'tf-roberta/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPa9W-ocewcr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
        "    vocab_file= ROBERTA_PATH +'vocab-roberta-base.json', \n",
        "    merges_file= ROBERTA_PATH +'merges-roberta-base.txt', \n",
        "    lowercase=True,\n",
        "    add_prefix_space=True\n",
        ")\n",
        "SENTIMENT_ID = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpYqp0mIeo4s",
        "colab_type": "text"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iTERuGfe6fJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "6a853b0a-0f41-4b9e-85e9-4528a66ee9cf"
      },
      "source": [
        "class Config():\n",
        "    def __init__(self, config_path=None):\n",
        "\n",
        "        if config_path:\n",
        "            with open(config_path,'r') as json_file:\n",
        "                self.__dict__ = json.load(json_file)\n",
        "        else:\n",
        "            \n",
        "            self.version='tf-tpu-v1'\n",
        "\n",
        "            # preprocessing\n",
        "            self.max_len = 120\n",
        "            # self.tweet_size_to_ignore = 1\n",
        "\n",
        "            # learning\n",
        "            self.drop_out = 0.3\n",
        "            self.conv1d_size_1 = 128\n",
        "            self.conv1d_size_2 = 64\n",
        "            self.lr = 3e-5\n",
        "            self.lr_decrease = 0.2\n",
        "            self.label_smoothing = 0.25\n",
        "            self.train_batch_size = 8\n",
        "            self.valid_batch_size = 16\n",
        "            self.num_epochs = 3\n",
        "            self.num_folds = 5\n",
        "            self.smart_span = True\n",
        "            # dev\n",
        "            self.num_folds_to_train = 1\n",
        "            self.dev = False\n",
        "            self.overwrite_model = True\n",
        "        \n",
        "        print(str(self.__dict__))\n",
        "\n",
        "    def get(self, var):\n",
        "        return getattr(self, var, None)\n",
        "\n",
        "\n",
        "# config_file_name = \"config-json-tpu-v28.txt\"\n",
        "# config_file_path = os.path.join(CHECKPOINT_PATH_HP_TUNING,config_file_name)\n",
        "# if os.path.exists(config_file_path):\n",
        "    # config = Config(config_file_path)\n",
        "# else:\n",
        "    # print(\"config file not found! Default configuration will be used\")\n",
        "config = Config()\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'version': 'tf-tpu-v1', 'max_len': 120, 'drop_out': 0.3, 'conv1d_size_1': 128, 'conv1d_size_2': 64, 'lr': 3e-05, 'lr_decrease': 0.2, 'label_smoothing': 0.25, 'train_batch_size': 8, 'valid_batch_size': 16, 'num_epochs': 3, 'num_folds': 5, 'smart_span': True, 'num_folds_to_train': 1, 'dev': False, 'overwrite_model': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6neYZwqEeq9T",
        "colab_type": "text"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ks9Sj0CuHWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_to_file(config, text):\n",
        "  try:\n",
        "    f = open(CHECKPOINT_PATH + config.version + \"_logs.txt\", 'a')\n",
        "  except:\n",
        "    f = open(CHECKPOINT_PATH + config.version + \"_logs.txt\", 'w')\n",
        "\n",
        "  print(text)\n",
        "  f.write(time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "  f.write(\"  -- \")\n",
        "  f.write(text)\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "def jaccard(str1, str2): \n",
        "\n",
        "    a = set(str(str1).lower().split()) \n",
        "    b = set(str(str2).lower().split())\n",
        "\n",
        "    c = a.intersection(b)\n",
        "\n",
        "    if (len(a) + len(b) - len(c)) > 0:\n",
        "      return float(len(c)) / (len(a) + len(b) - len(c))\n",
        "    else:\n",
        "      return 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIgKJIhegxRV",
        "colab_type": "text"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaj2JglXuVhQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def import_data(config):\n",
        "    def read_train():\n",
        "        train=pd.read_csv(DATA_PATH + 'train.csv')\n",
        "        train['text']=train['text'].astype(str)\n",
        "        train['selected_text']=train['selected_text'].astype(str)\n",
        "\n",
        "        return train\n",
        "\n",
        "    def read_test():\n",
        "        test=pd.read_csv(DATA_PATH + 'test.csv')\n",
        "        test['text']=test['text'].astype(str)\n",
        "\n",
        "        return test\n",
        "\n",
        "    if (config.dev == True):\n",
        "        train_df = read_train().sample(frac=.05, random_state=1).reset_index()\n",
        "        test_df = read_test().sample(frac=.05, random_state=1).reset_index()\n",
        "    else:\n",
        "        train_df = read_train()\n",
        "        test_df = read_test()\n",
        "\n",
        "\n",
        "    print_to_file(config, \"train_df shape:\" + str(train_df.shape))\n",
        "    print_to_file(config, \"test_df shape:\" + str(test_df.shape))\n",
        "    \n",
        "    return train_df, test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOZTcNnZhICe",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOXSqpFxuXGq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess_train(config, train_df):\n",
        "    ct = train_df.shape[0]\n",
        "    input_ids = np.ones((ct,config.max_len),dtype='int32')\n",
        "    attention_mask = np.zeros((ct,config.max_len),dtype='int32')\n",
        "    token_type_ids = np.zeros((ct,config.max_len),dtype='int32')\n",
        "    start_tokens = np.zeros((ct,config.max_len),dtype='int32')\n",
        "    end_tokens = np.zeros((ct,config.max_len),dtype='int32')\n",
        "\n",
        "    for k in range(train_df.shape[0]):\n",
        "        \n",
        "        # FIND OVERLAP\n",
        "        text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "        text2 = \" \".join(train_df.loc[k,'selected_text'].split())\n",
        "        idx = text1.find(text2)\n",
        "        chars = np.zeros((len(text1)))\n",
        "        chars[idx:idx+len(text2)]=1\n",
        "        if text1[idx-1]==' ': chars[idx-1] = 1 \n",
        "        enc = tokenizer.encode(text1) \n",
        "            \n",
        "        # ID_OFFSETS\n",
        "        offsets = []; idx=0\n",
        "        for t in enc.ids:\n",
        "            w = tokenizer.decode([t])\n",
        "            offsets.append((idx,idx+len(w)))\n",
        "            idx += len(w)\n",
        "        \n",
        "        # START END TOKENS\n",
        "        toks = []\n",
        "        for i,(a,b) in enumerate(offsets):\n",
        "            sm = np.sum(chars[a:b])\n",
        "            if sm>0: toks.append(i) \n",
        "            \n",
        "        s_tok = SENTIMENT_ID[train_df.loc[k,'sentiment']]\n",
        "        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
        "        attention_mask[k,:len(enc.ids)+5] = 1\n",
        "        if len(toks)>0:\n",
        "            start_tokens[k,toks[0]+1] = 1\n",
        "            end_tokens[k,toks[-1]+1] = 1\n",
        "    return  input_ids, attention_mask, token_type_ids, start_tokens, end_tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA_vXWpuucFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_dataset(config, idxT, idxV, input_ids, attention_mask, token_type_ids, start_tokens, end_tokens):\n",
        "    \n",
        "    # Trainset\n",
        "    trn_input_ids = input_ids[idxT,]\n",
        "    trn_att_mask = attention_mask[idxT,]\n",
        "    trn_token_type_ids = token_type_ids[idxT,]\n",
        "    \n",
        "    trn_start_tokens = start_tokens[idxT,]\n",
        "    trn_end_tokens = end_tokens[idxT,]\n",
        "    \n",
        "    # Validation set\n",
        "    val_input_ids = input_ids[idxV,]\n",
        "    val_att_mask = attention_mask[idxV,]\n",
        "    val_token_type_ids = token_type_ids[idxV,]\n",
        "    \n",
        "    val_start_tokens = start_tokens[idxV,]\n",
        "    val_end_tokens = end_tokens[idxV,]\n",
        "    \n",
        "    # Generating tf.data object\n",
        "    train_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices(({'input_ids':trn_input_ids, 'attention_mask': trn_att_mask, 'token_type_ids': trn_token_type_ids}, \n",
        "                             {'start_tokens': trn_start_tokens, 'end_tokens': trn_end_tokens}))\n",
        "        .shuffle(2048)\n",
        "        .batch(config.train_batch_size)\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    \n",
        "    valid_dataset = (\n",
        "        tf.data.Dataset\n",
        "        .from_tensor_slices(({'input_ids':val_input_ids, 'attention_mask': val_att_mask, 'token_type_ids': val_token_type_ids}, \n",
        "                             {'start_tokens': val_start_tokens, 'end_tokens': val_end_tokens}))\n",
        "        .batch(config.valid_batch_size)\n",
        "        .cache()\n",
        "        .prefetch(AUTO)\n",
        "    )\n",
        "    \n",
        "    return train_dataset, valid_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvUiECrcjbh6",
        "colab_type": "text"
      },
      "source": [
        "# Design Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t37zAgJTueNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheduler(epoch):\n",
        "    return config.lr * config.lr_decrease**epoch\n",
        "\n",
        "def build_model(config):\n",
        "    ids = tf.keras.layers.Input((config.max_len,), dtype=tf.int32, name='input_ids')\n",
        "    att = tf.keras.layers.Input((config.max_len,), dtype=tf.int32, name='attention_mask')\n",
        "    tok = tf.keras.layers.Input((config.max_len,), dtype=tf.int32, name='token_type_ids')\n",
        "\n",
        "    robert_config = RobertaConfig.from_pretrained(ROBERTA_PATH + 'config-roberta-base.json')\n",
        "    bert_model = TFRobertaModel.from_pretrained(ROBERTA_PATH + 'pretrained-roberta-base.h5',config=robert_config)\n",
        "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
        "    \n",
        "    \n",
        "    x1 = tf.keras.layers.Dropout(config.drop_out)(x[0]) \n",
        "    x1 = tf.keras.layers.Conv1D(config.conv1d_size_1, 2, padding='same')(x1)\n",
        "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
        "    x1 = tf.keras.layers.Conv1D(config.conv1d_size_2, 2, padding='same')(x1)\n",
        "    x1 = tf.keras.layers.Dense(1)(x1)\n",
        "    x1 = tf.keras.layers.Flatten()(x1)\n",
        "    x1 = tf.keras.layers.Activation('softmax', name='start_tokens')(x1)\n",
        "    \n",
        "    x2 = tf.keras.layers.Dropout(config.drop_out)(x[0]) \n",
        "    x2 = tf.keras.layers.Conv1D(config.conv1d_size_1, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
        "    x2 = tf.keras.layers.Conv1D(config.conv1d_size_2, 2, padding='same')(x2)\n",
        "    x2 = tf.keras.layers.Dense(1)(x2)\n",
        "    x2 = tf.keras.layers.Flatten()(x2)\n",
        "    x2 = tf.keras.layers.Activation('softmax', name='end_tokens')(x2)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ex5nAdzjdtb",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KlUcgkbjgcR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_train(config, train_df, input_ids, attention_mask, token_type_ids, start_tokens, end_tokens):\n",
        "    jac = []\n",
        "    oof_start = np.zeros((input_ids.shape[0],config.max_len))\n",
        "    oof_end = np.zeros((input_ids.shape[0],config.max_len))\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=config.num_folds, shuffle=True, random_state=777)\n",
        "    for fold, (idxT,idxV) in enumerate(skf.split(input_ids, train_df.sentiment.values)):\n",
        "        if fold < config.num_folds_to_train:\n",
        "            print_to_file(config, '#'*25)\n",
        "            print_to_file(config, '### FOLD %i'%(fold+1))\n",
        "            print_to_file(config, '#'*25)\n",
        "\n",
        "            trn_dataset, val_dataset = generate_dataset(config, idxT, idxV, input_ids, attention_mask, token_type_ids, start_tokens, end_tokens)\n",
        "\n",
        "            if config.overwrite_model or (not os.path.exists(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(config.version,fold))):\n",
        "                K.clear_session()\n",
        "                with strategy.scope():\n",
        "                    model = build_model(config)\n",
        "                    model.compile(\n",
        "                        optimizer = tf.keras.optimizers.Adam(learning_rate=config.lr),\n",
        "                        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=config.label_smoothing)\n",
        "                        )\n",
        "            \n",
        "                reduce_lr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: config.lr * config.lr_decrease**epoch, verbose=1)\n",
        "\n",
        "                sv = tf.keras.callbacks.ModelCheckpoint(\n",
        "                    CHECKPOINT_PATH + '%s-roberta-%i.h5'%(config.version,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
        "                    save_weights_only=True, mode='auto', save_freq='epoch')\n",
        "                hist = model.fit(\n",
        "                    trn_dataset,\n",
        "                    epochs=config.num_epochs,\n",
        "                    batch_size=config.train_batch_size,\n",
        "                    verbose=1,\n",
        "                    callbacks=[sv, reduce_lr],\n",
        "                    validation_data=val_dataset\n",
        "                    )\n",
        "            else:\n",
        "                print_to_file(config, \"training Fold %d is skipped!\"%(fold+1))\n",
        "\n",
        "            print_to_file(config, 'Loading model...')\n",
        "            model.load_weights(CHECKPOINT_PATH + '%s-roberta-%i.h5'%(config.version,fold))\n",
        "            \n",
        "            print_to_file(config, 'Predicting OOF...')\n",
        "            oof_start[idxV,],oof_end[idxV,] = model.predict(val_dataset, verbose=1)\n",
        "            \n",
        "            # DISPLAY FOLD JACCARD\n",
        "            all = []\n",
        "            for k in idxV:\n",
        "                a = np.argmax(oof_start[k,])\n",
        "                b = np.argmax(oof_end[k,])\n",
        "\n",
        "                if (a>b) and config.smart_span: \n",
        "                    max_len = len(oof_start[k,])\n",
        "                    a = np.tile(oof_start[k,], (max_len, 1))\n",
        "                    b = np.tile(oof_end[k,], (max_len, 1))\n",
        "                    c = np.tril(a + b.T, k=0).T\n",
        "                    c[c == 0] = -1000\n",
        "                    a = np.unravel_index(c.argmax(), c.shape)[0]\n",
        "                    b = np.unravel_index(c.argmax(), c.shape)[1]\n",
        "\n",
        "                if (a>b):\n",
        "                    st = train_df.loc[k,'text'] \n",
        "                else:\n",
        "                    text1 = \" \"+\" \".join(train_df.loc[k,'text'].split())\n",
        "                    enc = tokenizer.encode(text1)\n",
        "                    st = tokenizer.decode(enc.ids[a-1:b])\n",
        "                \n",
        "                train_df.loc[k,'selected_text_predicted'] = st\n",
        "                train_df.loc[k,'jaccard'] = jaccard(train_df.loc[k,'selected_text_predicted'],train_df.loc[k,'selected_text'])\n",
        "                all.append(train_df.loc[k,'jaccard'])\n",
        "\n",
        "            jac.append(np.mean(all))\n",
        "            print_to_file(config, '>>>> FOLD %i Jaccard = '%(fold+1) + str(np.mean(all)))\n",
        "\n",
        "    K.clear_session()\n",
        "    print_to_file(config, 'Overall averrage of Jaccards in %i folds = '% (fold+1) + str(np.mean(jac)))\n",
        "    return np.mean(jac)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dev0RlUWf4PY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a5e64387-aaa7-400a-834a-df4df09d03fa"
      },
      "source": [
        "list_lr = [5e-6, 1e-5, 2e-5, 3e-5, 6e-5, 9e-5]\n",
        "list_lr_decrease = [.1, .2, .5, .7]\n",
        "list_dropout = [0, .1, .2, .3, .4, .5]\n",
        "list_conv1d_size_1 = [64, 128, 256, 512]\n",
        "list_conv1d_size_2 = [8, 16, 32, 64]\n",
        "list_label_smoothing = [0, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5]\n",
        "list_num_epochs = [3,4,5]\n",
        "\n",
        "session_num = 0\n",
        "random.seed(67876)\n",
        "for i in range(100):\n",
        "    K.clear_session()\n",
        "    config = Config()\n",
        "    config.dev = False\n",
        "    config.num_folds_to_train = 2\n",
        "    config.overwrite_model = True\n",
        "    config.train_batch_size = 8\n",
        "    config.valid_batch_size = 16\n",
        "    config.num_folds = 5\n",
        "    config.smart_span = True\n",
        "\n",
        "    config.lr = random.choice(list_lr)\n",
        "    config.lr_decrease = random.choice(list_lr_decrease)\n",
        "    config.drop_out = random.choice(list_dropout)\n",
        "    config.conv1d_size_1 = random.choice(list_conv1d_size_1)\n",
        "    config.conv1d_size_2 = random.choice(list_conv1d_size_2)\n",
        "    config.label_smoothing = random.choice(list_label_smoothing)\n",
        "    config.num_epochs = random.choice(list_num_epochs)\n",
        "    config.version = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    run_name = \"run-%d\" % session_num\n",
        "    df = pd.DataFrame(config.__dict__, index=[0])\n",
        "    flag_continue = True\n",
        "    df_hist = pd.DataFrame()\n",
        "    if os.path.exists(CHECKPOINT_PATH + \"tensorboard/config_logs.csv\"):\n",
        "        df_hist = pd.read_csv(CHECKPOINT_PATH + \"tensorboard/config_logs.csv\")\n",
        "        intersected_df = pd.merge(\n",
        "            df.drop(\"version\",axis=1),\n",
        "            df_hist.drop(\"version\",axis=1).drop(\"score\",axis=1),\n",
        "            how='inner'\n",
        "            )\n",
        "        if intersected_df.shape[0] > 0:\n",
        "            print(\"this setting already exists!\")\n",
        "            flag_continue = False\n",
        "\n",
        "    else:\n",
        "        print(\"no history exists\")\n",
        "\n",
        "    if flag_continue:\n",
        "        print(df.iloc[0])\n",
        "        \n",
        "        train_df, test_df = import_data(config)\n",
        "        input_ids, attention_mask, token_type_ids, start_tokens, end_tokens = preprocess_train(config, train_df)\n",
        "        score = run_train(config, train_df, input_ids, attention_mask, token_type_ids, start_tokens, end_tokens)\n",
        "        \n",
        "        df[\"score\"] = score\n",
        "        if df_hist.shape[0] > 0:\n",
        "            df_full = pd.concat([df_hist,df])\n",
        "        else: \n",
        "            df_full = df\n",
        "        print(\"Final Score for VER %s is: %.5f\" % (config.version, score))\n",
        "        df_full.to_csv(CHECKPOINT_PATH + \"tensorboard/config_logs.csv\", index=False)\n",
        "        print(config.__dict__)\n",
        "\n",
        "\n",
        "    # print('--- Starting trial: %s - %s' % (run_name,config.version))\n",
        "\n",
        "    # run('logs/hparam_tuning/' + run_name, hparams)\n",
        "    session_num += 1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'version': 'tf-tpu-v1', 'max_len': 120, 'drop_out': 0.3, 'conv1d_size_1': 128, 'conv1d_size_2': 64, 'lr': 3e-05, 'lr_decrease': 0.2, 'label_smoothing': 0.25, 'train_batch_size': 8, 'valid_batch_size': 16, 'num_epochs': 3, 'num_folds': 5, 'smart_span': True, 'num_folds_to_train': 1, 'dev': False, 'overwrite_model': True}\n",
            "this setting already exists!\n",
            "{'version': 'tf-tpu-v1', 'max_len': 120, 'drop_out': 0.3, 'conv1d_size_1': 128, 'conv1d_size_2': 64, 'lr': 3e-05, 'lr_decrease': 0.2, 'label_smoothing': 0.25, 'train_batch_size': 8, 'valid_batch_size': 16, 'num_epochs': 3, 'num_folds': 5, 'smart_span': True, 'num_folds_to_train': 1, 'dev': False, 'overwrite_model': True}\n",
            "this setting already exists!\n",
            "{'version': 'tf-tpu-v1', 'max_len': 120, 'drop_out': 0.3, 'conv1d_size_1': 128, 'conv1d_size_2': 64, 'lr': 3e-05, 'lr_decrease': 0.2, 'label_smoothing': 0.25, 'train_batch_size': 8, 'valid_batch_size': 16, 'num_epochs': 3, 'num_folds': 5, 'smart_span': True, 'num_folds_to_train': 1, 'dev': False, 'overwrite_model': True}\n",
            "this setting already exists!\n",
            "{'version': 'tf-tpu-v1', 'max_len': 120, 'drop_out': 0.3, 'conv1d_size_1': 128, 'conv1d_size_2': 64, 'lr': 3e-05, 'lr_decrease': 0.2, 'label_smoothing': 0.25, 'train_batch_size': 8, 'valid_batch_size': 16, 'num_epochs': 3, 'num_folds': 5, 'smart_span': True, 'num_folds_to_train': 1, 'dev': False, 'overwrite_model': True}\n",
            "this setting already exists!\n",
            "{'version': 'tf-tpu-v1', 'max_len': 120, 'drop_out': 0.3, 'conv1d_size_1': 128, 'conv1d_size_2': 64, 'lr': 3e-05, 'lr_decrease': 0.2, 'label_smoothing': 0.25, 'train_batch_size': 8, 'valid_batch_size': 16, 'num_epochs': 3, 'num_folds': 5, 'smart_span': True, 'num_folds_to_train': 1, 'dev': False, 'overwrite_model': True}\n",
            "this setting already exists!\n",
            "{'version': 'tf-tpu-v1', 'max_len': 120, 'drop_out': 0.3, 'conv1d_size_1': 128, 'conv1d_size_2': 64, 'lr': 3e-05, 'lr_decrease': 0.2, 'label_smoothing': 0.25, 'train_batch_size': 8, 'valid_batch_size': 16, 'num_epochs': 3, 'num_folds': 5, 'smart_span': True, 'num_folds_to_train': 1, 'dev': False, 'overwrite_model': True}\n",
            "version               20200504-071047\n",
            "max_len                           120\n",
            "drop_out                          0.2\n",
            "conv1d_size_1                     128\n",
            "conv1d_size_2                      16\n",
            "lr                              5e-06\n",
            "lr_decrease                       0.7\n",
            "label_smoothing                   0.4\n",
            "train_batch_size                    8\n",
            "valid_batch_size                   16\n",
            "num_epochs                          5\n",
            "num_folds                           5\n",
            "smart_span                       True\n",
            "num_folds_to_train                  2\n",
            "dev                             False\n",
            "overwrite_model                  True\n",
            "Name: 0, dtype: object\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/merge.py:1108: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation\n",
            "  UserWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train_df shape:(27481, 4)\n",
            "test_df shape:(3534, 3)\n",
            "#########################\n",
            "### FOLD 1\n",
            "#########################\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 5e-06.\n",
            "Epoch 1/5\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model/roberta/pooler/dense/kernel:0', 'tf_roberta_model/roberta/pooler/dense/bias:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2748/2748 [==============================] - ETA: 0s - start_tokens_loss: 0.9874 - loss: 1.9747 - end_tokens_loss: 0.9874\n",
            "Epoch 00001: val_loss improved from inf to 1.96868, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/20200504-071047-roberta-0.h5\n",
            "2748/2748 [==============================] - 247s 90ms/step - start_tokens_loss: 0.9874 - loss: 1.9747 - end_tokens_loss: 0.9874 - val_start_tokens_loss: 0.9843 - val_loss: 1.9687 - val_end_tokens_loss: 0.9844 - lr: 5.0000e-06\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 3.5e-06.\n",
            "Epoch 2/5\n",
            "2748/2748 [==============================] - ETA: 0s - start_tokens_loss: 0.9855 - loss: 1.9711 - end_tokens_loss: 0.9856\n",
            "Epoch 00002: val_loss improved from 1.96868 to 1.96840, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/20200504-071047-roberta-0.h5\n",
            "2748/2748 [==============================] - 245s 89ms/step - start_tokens_loss: 0.9855 - loss: 1.9711 - end_tokens_loss: 0.9856 - val_start_tokens_loss: 0.9842 - val_loss: 1.9684 - val_end_tokens_loss: 0.9842 - lr: 3.5000e-06\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 2.45e-06.\n",
            "Epoch 3/5\n",
            "2748/2748 [==============================] - ETA: 0s - start_tokens_loss: 0.9854 - loss: 1.9707 - end_tokens_loss: 0.9853\n",
            "Epoch 00003: val_loss improved from 1.96840 to 1.96830, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/20200504-071047-roberta-0.h5\n",
            "2748/2748 [==============================] - 248s 90ms/step - start_tokens_loss: 0.9854 - loss: 1.9707 - end_tokens_loss: 0.9853 - val_start_tokens_loss: 0.9842 - val_loss: 1.9683 - val_end_tokens_loss: 0.9841 - lr: 2.4500e-06\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 1.7149999999999997e-06.\n",
            "Epoch 4/5\n",
            "2748/2748 [==============================] - ETA: 0s - start_tokens_loss: 0.9853 - loss: 1.9705 - end_tokens_loss: 0.9852\n",
            "Epoch 00004: val_loss improved from 1.96830 to 1.96829, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/20200504-071047-roberta-0.h5\n",
            "2748/2748 [==============================] - 252s 92ms/step - start_tokens_loss: 0.9853 - loss: 1.9705 - end_tokens_loss: 0.9852 - val_start_tokens_loss: 0.9842 - val_loss: 1.9683 - val_end_tokens_loss: 0.9841 - lr: 1.7150e-06\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 1.2004999999999998e-06.\n",
            "Epoch 5/5\n",
            "2748/2748 [==============================] - ETA: 0s - start_tokens_loss: 0.9852 - loss: 1.9704 - end_tokens_loss: 0.9852\n",
            "Epoch 00005: val_loss improved from 1.96829 to 1.96827, saving model to /content/drive/My Drive/input/model_checkpoint/roberta/20200504-071047-roberta-0.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6fjdqot8fCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# /usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value)\n",
        "\n",
        "# ResourceExhaustedError: 8 root error(s) found.\n",
        "#   (0) Resource exhausted: {{function_node __inference_train_function_790469}} Attempting to reserve 355.95M at the bottom of memory. That was not possible. There are 462.88M free, 0B reserved, and 280.95M reservable.\n",
        "# \t [[{{node cluster_train_function/_execute_4_0}}]]\n",
        "# Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
        "\n",
        "#   (1) Resource exhausted: {{function_node __inference_train_function_790469}} Attempting to reserve 355.95M at the bottom of memory. That was not possible. There are 462.88M free, 0B reserved, and 280.95M reservable.\n",
        "# \t [[{{node cluster_train_function/_execute_6_0}}]]\n",
        "# Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
        "\n",
        "#   (2) Resource exhausted: {{function_node __inference_train_function_790469}} Attempting to reserve 355.95M at the bottom of memory. That was not possible. There are 462.88M free, 0B reserved, and 280.98M reservable.\n",
        "# \t [[{{node cluster_train_function/_execute_1_0}}]]\n",
        "# Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
        "\n",
        "#   (3) Cancelled: {{function_node __inference_train_function_790469}} RPC cancelled, not running TPU program on device 7\n",
        "# \t [[{{node cluster_train_function/_execute_7_0}}]]\n",
        "#   (4) Cancelled: {{function_node __inference_train_function_790469}} RPC cancelled, not running TPU program on device 5\n",
        "# \t [[{{node cluster_train_function/_execute_5_0}}]]\n",
        "#   (5) Cancelled: {{function_node __inference_train_function_790469}} RPC cancelled, not running TPU program on device 2\n",
        "# \t [[{{node cluster_train_function/_execute_2_0}}]]\n",
        "#   (6) Cancelled: {{function_node __inference_train_function_790469}} RPC cancelled, not running TPU program on device 0\n",
        "# \t [[{{node cluster_train_function/_execute_0_0}}]]\n",
        "#   (7) Resource exhausted: {{function_node __inference_train_function_790469}} Attempting to reserve 355.95M at the bottom of memory. That was not possible. There are 462.88M free, 0B reserved, and 280.95M reservable.\n",
        "# \t [[{{node cluster_train_function/_execute_3_0}}]]\n",
        "# Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
        "\n",
        "# 0 successful operations.\n",
        "# 1 derived errors ignored."
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}