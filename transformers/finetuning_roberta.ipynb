{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finetuning_roberta.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNqvgtcXRtCt275bChp+9mX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/branjbar/nlp-bert-family/blob/master/transformers/finetuning_roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOkYyvysn0WN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://huggingface.co/transformers/examples.html#roberta-bert-and-masked-language-modeling"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M75gwNjNd1Jo",
        "colab_type": "code",
        "outputId": "97d290b7-9596-49b2-d58b-d867cba21784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "Tue Apr 28 15:46:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 13.7 gigabytes of available RAM\n",
            "\n",
            "To enable a high-RAM runtime, select the Runtime → \"Change runtime type\"\n",
            "menu, and then select High-RAM in the Runtime shape dropdown. Then, \n",
            "re-execute this cell.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38-dlU3QeFyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xiYtgm-gGAM",
        "colab_type": "code",
        "outputId": "80671b7e-dc6b-423b-ac7f-433f7cc53b69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (79/79), done.\u001b[K\n",
            "remote: Total 24978 (delta 43), reused 23 (delta 4), pack-reused 24889\u001b[K\n",
            "Receiving objects: 100% (24978/24978), 15.20 MiB | 9.20 MiB/s, done.\n",
            "Resolving deltas: 100% (17522/17522), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyvgfYz9iTKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"transformers\")\n",
        "!pip install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE48c5YAle4E",
        "colab_type": "code",
        "outputId": "ff4c21c1-a80c-4c41-c782-fdebf00b6c92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "!wget https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
        "!unzip wikitext-2-raw-v1.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-28 16:20:00--  https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-raw-v1.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.13.102\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.13.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4721645 (4.5M) [application/zip]\n",
            "Saving to: ‘wikitext-2-raw-v1.zip’\n",
            "\n",
            "wikitext-2-raw-v1.z 100%[===================>]   4.50M  3.06MB/s    in 1.5s    \n",
            "\n",
            "2020-04-28 16:20:03 (3.06 MB/s) - ‘wikitext-2-raw-v1.zip’ saved [4721645/4721645]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6g3HHcIepjt",
        "colab_type": "code",
        "outputId": "49cdc989-2a61-4bb0-a411-ed433d6e3715",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python examples/run_language_modeling.py \\\n",
        "    --output_dir=output \\\n",
        "    --model_type=roberta \\\n",
        "    --model_name_or_path=roberta-base \\\n",
        "    --do_train \\\n",
        "    --train_data_file=wikitext-2-raw/wiki.test.raw \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=wikitext-2-raw/wiki.test.raw \\\n",
        "    --mlm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-28 16:20:56.756318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "04/28/2020 16:20:58 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "04/28/2020 16:20:58 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/28/2020 16:20:58 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=None, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1)\n",
            "04/28/2020 16:20:59 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "04/28/2020 16:20:59 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/28/2020 16:21:00 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "04/28/2020 16:21:00 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/28/2020 16:21:02 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "04/28/2020 16:21:02 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "04/28/2020 16:21:02 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "04/28/2020 16:21:09 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "04/28/2020 16:21:09 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at wikitext-2-raw\n",
            "04/28/2020 16:21:11 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file wikitext-2-raw/cached_lm_RobertaTokenizer_510_wiki.test.raw [took 0.009 s]\n",
            "04/28/2020 16:21:11 - INFO - transformers.data.datasets.language_modeling -   Loading features from cached file wikitext-2-raw/cached_lm_RobertaTokenizer_510_wiki.test.raw [took 0.009 s]\n",
            "04/28/2020 16:21:26 - INFO - transformers.trainer -   ***** Running training *****\n",
            "04/28/2020 16:21:26 - INFO - transformers.trainer -     Num examples = 561\n",
            "04/28/2020 16:21:26 - INFO - transformers.trainer -     Num Epochs = 3\n",
            "04/28/2020 16:21:26 - INFO - transformers.trainer -     Instantaneous batch size per GPU = 8\n",
            "04/28/2020 16:21:26 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "04/28/2020 16:21:26 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "04/28/2020 16:21:26 - INFO - transformers.trainer -     Total optimization steps = 213\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/71 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/71 [00:00<01:04,  1.09it/s]\u001b[A\n",
            "Iteration:   3% 2/71 [00:01<00:57,  1.21it/s]\u001b[A\n",
            "Iteration:   4% 3/71 [00:02<00:52,  1.30it/s]\u001b[A\n",
            "Iteration:   6% 4/71 [00:02<00:48,  1.38it/s]\u001b[A\n",
            "Iteration:   7% 5/71 [00:03<00:45,  1.44it/s]\u001b[A\n",
            "Iteration:   8% 6/71 [00:04<00:43,  1.49it/s]\u001b[A\n",
            "Iteration:  10% 7/71 [00:04<00:42,  1.52it/s]\u001b[A\n",
            "Iteration:  11% 8/71 [00:05<00:40,  1.55it/s]\u001b[A\n",
            "Iteration:  13% 9/71 [00:05<00:39,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 10/71 [00:06<00:38,  1.57it/s]\u001b[A\n",
            "Iteration:  15% 11/71 [00:07<00:37,  1.58it/s]\u001b[A\n",
            "Iteration:  17% 12/71 [00:07<00:37,  1.59it/s]\u001b[A\n",
            "Iteration:  18% 13/71 [00:08<00:36,  1.59it/s]\u001b[A\n",
            "Iteration:  20% 14/71 [00:09<00:35,  1.59it/s]\u001b[A\n",
            "Iteration:  21% 15/71 [00:09<00:35,  1.59it/s]\u001b[A\n",
            "Iteration:  23% 16/71 [00:10<00:34,  1.60it/s]\u001b[A\n",
            "Iteration:  24% 17/71 [00:10<00:33,  1.60it/s]\u001b[A\n",
            "Iteration:  25% 18/71 [00:11<00:33,  1.60it/s]\u001b[A\n",
            "Iteration:  27% 19/71 [00:12<00:32,  1.60it/s]\u001b[A\n",
            "Iteration:  28% 20/71 [00:12<00:31,  1.60it/s]\u001b[A\n",
            "Iteration:  30% 21/71 [00:13<00:31,  1.60it/s]\u001b[A\n",
            "Iteration:  31% 22/71 [00:14<00:30,  1.60it/s]\u001b[A\n",
            "Iteration:  32% 23/71 [00:14<00:30,  1.60it/s]\u001b[A\n",
            "Iteration:  34% 24/71 [00:15<00:29,  1.60it/s]\u001b[A\n",
            "Iteration:  35% 25/71 [00:15<00:28,  1.60it/s]\u001b[A\n",
            "Iteration:  37% 26/71 [00:16<00:28,  1.60it/s]\u001b[A\n",
            "Iteration:  38% 27/71 [00:17<00:27,  1.60it/s]\u001b[A\n",
            "Iteration:  39% 28/71 [00:17<00:26,  1.60it/s]\u001b[A\n",
            "Iteration:  41% 29/71 [00:18<00:26,  1.61it/s]\u001b[A\n",
            "Iteration:  42% 30/71 [00:19<00:25,  1.61it/s]\u001b[A\n",
            "Iteration:  44% 31/71 [00:19<00:24,  1.61it/s]\u001b[A\n",
            "Iteration:  45% 32/71 [00:20<00:24,  1.60it/s]\u001b[A\n",
            "Iteration:  46% 33/71 [00:20<00:23,  1.60it/s]\u001b[A\n",
            "Iteration:  48% 34/71 [00:21<00:23,  1.60it/s]\u001b[A\n",
            "Iteration:  49% 35/71 [00:22<00:22,  1.60it/s]\u001b[A\n",
            "Iteration:  51% 36/71 [00:22<00:21,  1.60it/s]\u001b[A\n",
            "Iteration:  52% 37/71 [00:23<00:21,  1.61it/s]\u001b[A\n",
            "Iteration:  54% 38/71 [00:24<00:20,  1.61it/s]\u001b[A\n",
            "Iteration:  55% 39/71 [00:24<00:19,  1.60it/s]\u001b[A\n",
            "Iteration:  56% 40/71 [00:25<00:19,  1.61it/s]\u001b[A\n",
            "Iteration:  58% 41/71 [00:25<00:18,  1.61it/s]\u001b[A\n",
            "Iteration:  59% 42/71 [00:26<00:18,  1.61it/s]\u001b[A\n",
            "Iteration:  61% 43/71 [00:27<00:17,  1.61it/s]\u001b[A\n",
            "Iteration:  62% 44/71 [00:27<00:16,  1.61it/s]\u001b[A\n",
            "Iteration:  63% 45/71 [00:28<00:16,  1.61it/s]\u001b[A\n",
            "Iteration:  65% 46/71 [00:28<00:15,  1.61it/s]\u001b[A\n",
            "Iteration:  66% 47/71 [00:29<00:14,  1.61it/s]\u001b[A\n",
            "Iteration:  68% 48/71 [00:30<00:14,  1.61it/s]\u001b[A\n",
            "Iteration:  69% 49/71 [00:30<00:13,  1.61it/s]\u001b[A\n",
            "Iteration:  70% 50/71 [00:31<00:13,  1.61it/s]\u001b[A\n",
            "Iteration:  72% 51/71 [00:32<00:12,  1.61it/s]\u001b[A\n",
            "Iteration:  73% 52/71 [00:32<00:11,  1.60it/s]\u001b[A\n",
            "Iteration:  75% 53/71 [00:33<00:11,  1.60it/s]\u001b[A\n",
            "Iteration:  76% 54/71 [00:33<00:10,  1.61it/s]\u001b[A\n",
            "Iteration:  77% 55/71 [00:34<00:10,  1.60it/s]\u001b[A\n",
            "Iteration:  79% 56/71 [00:35<00:09,  1.60it/s]\u001b[A\n",
            "Iteration:  80% 57/71 [00:35<00:08,  1.60it/s]\u001b[A\n",
            "Iteration:  82% 58/71 [00:36<00:08,  1.60it/s]\u001b[A\n",
            "Iteration:  83% 59/71 [00:37<00:07,  1.60it/s]\u001b[A\n",
            "Iteration:  85% 60/71 [00:37<00:06,  1.61it/s]\u001b[A\n",
            "Iteration:  86% 61/71 [00:38<00:06,  1.61it/s]\u001b[A\n",
            "Iteration:  87% 62/71 [00:38<00:05,  1.61it/s]\u001b[A\n",
            "Iteration:  89% 63/71 [00:39<00:04,  1.61it/s]\u001b[A\n",
            "Iteration:  90% 64/71 [00:40<00:04,  1.61it/s]\u001b[A\n",
            "Iteration:  92% 65/71 [00:40<00:03,  1.61it/s]\u001b[A\n",
            "Iteration:  93% 66/71 [00:41<00:03,  1.61it/s]\u001b[A\n",
            "Iteration:  94% 67/71 [00:42<00:02,  1.61it/s]\u001b[A\n",
            "Iteration:  96% 68/71 [00:42<00:01,  1.61it/s]\u001b[A\n",
            "Iteration:  97% 69/71 [00:43<00:01,  1.61it/s]\u001b[A\n",
            "Iteration:  99% 70/71 [00:43<00:00,  1.60it/s]\u001b[A\n",
            "Iteration: 100% 71/71 [00:44<00:00,  1.61it/s]\n",
            "Epoch:  33% 1/3 [00:44<01:28, 44.06s/it]\n",
            "Iteration:   0% 0/71 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/71 [00:00<00:43,  1.61it/s]\u001b[A\n",
            "Iteration:   3% 2/71 [00:01<00:42,  1.61it/s]\u001b[A\n",
            "Iteration:   4% 3/71 [00:01<00:42,  1.60it/s]\u001b[A\n",
            "Iteration:   6% 4/71 [00:02<00:41,  1.60it/s]\u001b[A\n",
            "Iteration:   7% 5/71 [00:03<00:41,  1.60it/s]\u001b[A\n",
            "Iteration:   8% 6/71 [00:03<00:40,  1.60it/s]\u001b[A\n",
            "Iteration:  10% 7/71 [00:04<00:39,  1.61it/s]\u001b[A\n",
            "Iteration:  11% 8/71 [00:04<00:39,  1.61it/s]\u001b[A\n",
            "Iteration:  13% 9/71 [00:05<00:38,  1.61it/s]\u001b[A\n",
            "Iteration:  14% 10/71 [00:06<00:37,  1.61it/s]\u001b[A\n",
            "Iteration:  15% 11/71 [00:06<00:37,  1.61it/s]\u001b[A\n",
            "Iteration:  17% 12/71 [00:07<00:36,  1.60it/s]\u001b[A\n",
            "Iteration:  18% 13/71 [00:08<00:36,  1.60it/s]\u001b[A\n",
            "Iteration:  20% 14/71 [00:08<00:35,  1.60it/s]\u001b[A\n",
            "Iteration:  21% 15/71 [00:09<00:34,  1.61it/s]\u001b[A\n",
            "Iteration:  23% 16/71 [00:09<00:34,  1.59it/s]\u001b[A\n",
            "Iteration:  24% 17/71 [00:10<00:33,  1.60it/s]\u001b[A\n",
            "Iteration:  25% 18/71 [00:11<00:33,  1.60it/s]\u001b[A\n",
            "Iteration:  27% 19/71 [00:11<00:32,  1.60it/s]\u001b[A\n",
            "Iteration:  28% 20/71 [00:12<00:31,  1.60it/s]\u001b[A\n",
            "Iteration:  30% 21/71 [00:13<00:31,  1.61it/s]\u001b[A\n",
            "Iteration:  31% 22/71 [00:13<00:30,  1.61it/s]\u001b[A\n",
            "Iteration:  32% 23/71 [00:14<00:29,  1.61it/s]\u001b[A\n",
            "Iteration:  34% 24/71 [00:14<00:29,  1.61it/s]\u001b[A\n",
            "Iteration:  35% 25/71 [00:15<00:28,  1.61it/s]\u001b[A\n",
            "Iteration:  37% 26/71 [00:16<00:27,  1.61it/s]\u001b[A\n",
            "Iteration:  38% 27/71 [00:16<00:27,  1.61it/s]\u001b[A\n",
            "Iteration:  39% 28/71 [00:17<00:26,  1.61it/s]\u001b[A\n",
            "Iteration:  41% 29/71 [00:18<00:26,  1.60it/s]\u001b[A\n",
            "Iteration:  42% 30/71 [00:18<00:25,  1.60it/s]\u001b[A\n",
            "Iteration:  44% 31/71 [00:19<00:24,  1.60it/s]\u001b[A\n",
            "Iteration:  45% 32/71 [00:19<00:24,  1.60it/s]\u001b[A\n",
            "Iteration:  46% 33/71 [00:20<00:23,  1.60it/s]\u001b[A\n",
            "Iteration:  48% 34/71 [00:21<00:23,  1.61it/s]\u001b[A\n",
            "Iteration:  49% 35/71 [00:21<00:22,  1.60it/s]\u001b[A\n",
            "Iteration:  51% 36/71 [00:22<00:21,  1.61it/s]\u001b[A\n",
            "Iteration:  52% 37/71 [00:23<00:21,  1.61it/s]\u001b[A\n",
            "Iteration:  54% 38/71 [00:23<00:20,  1.61it/s]\u001b[A\n",
            "Iteration:  55% 39/71 [00:24<00:19,  1.60it/s]\u001b[A\n",
            "Iteration:  56% 40/71 [00:24<00:19,  1.60it/s]\u001b[A\n",
            "Iteration:  58% 41/71 [00:25<00:18,  1.60it/s]\u001b[A\n",
            "Iteration:  59% 42/71 [00:26<00:18,  1.61it/s]\u001b[A\n",
            "Iteration:  61% 43/71 [00:26<00:17,  1.61it/s]\u001b[A\n",
            "Iteration:  62% 44/71 [00:27<00:16,  1.61it/s]\u001b[A\n",
            "Iteration:  63% 45/71 [00:28<00:16,  1.61it/s]\u001b[A\n",
            "Iteration:  65% 46/71 [00:28<00:15,  1.60it/s]\u001b[A\n",
            "Iteration:  66% 47/71 [00:29<00:14,  1.60it/s]\u001b[A\n",
            "Iteration:  68% 48/71 [00:29<00:14,  1.61it/s]\u001b[A\n",
            "Iteration:  69% 49/71 [00:30<00:13,  1.61it/s]\u001b[A\n",
            "Iteration:  70% 50/71 [00:31<00:13,  1.61it/s]\u001b[A\n",
            "Iteration:  72% 51/71 [00:31<00:12,  1.61it/s]\u001b[A\n",
            "Iteration:  73% 52/71 [00:32<00:11,  1.61it/s]\u001b[A\n",
            "Iteration:  75% 53/71 [00:33<00:11,  1.61it/s]\u001b[A\n",
            "Iteration:  76% 54/71 [00:33<00:10,  1.61it/s]\u001b[A\n",
            "Iteration:  77% 55/71 [00:34<00:09,  1.61it/s]\u001b[A\n",
            "Iteration:  79% 56/71 [00:34<00:09,  1.61it/s]\u001b[A\n",
            "Iteration:  80% 57/71 [00:35<00:08,  1.61it/s]\u001b[A\n",
            "Iteration:  82% 58/71 [00:36<00:08,  1.61it/s]\u001b[A\n",
            "Iteration:  83% 59/71 [00:36<00:07,  1.61it/s]\u001b[A\n",
            "Iteration:  85% 60/71 [00:37<00:06,  1.60it/s]\u001b[A\n",
            "Iteration:  86% 61/71 [00:38<00:06,  1.61it/s]\u001b[A\n",
            "Iteration:  87% 62/71 [00:38<00:05,  1.60it/s]\u001b[A\n",
            "Iteration:  89% 63/71 [00:39<00:04,  1.60it/s]\u001b[A\n",
            "Iteration:  90% 64/71 [00:39<00:04,  1.60it/s]\u001b[A\n",
            "Iteration:  92% 65/71 [00:40<00:03,  1.60it/s]\u001b[A\n",
            "Iteration:  93% 66/71 [00:41<00:03,  1.60it/s]\u001b[A\n",
            "Iteration:  94% 67/71 [00:41<00:02,  1.61it/s]\u001b[A\n",
            "Iteration:  96% 68/71 [00:42<00:01,  1.61it/s]\u001b[A\n",
            "Iteration:  97% 69/71 [00:42<00:01,  1.61it/s]\u001b[A\n",
            "Iteration:  99% 70/71 [00:43<00:00,  1.61it/s]\u001b[A\n",
            "Iteration: 100% 71/71 [00:43<00:00,  1.62it/s]\n",
            "Epoch:  67% 2/3 [01:27<00:43, 43.97s/it]\n",
            "Iteration:   0% 0/71 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   1% 1/71 [00:00<00:43,  1.60it/s]\u001b[A\n",
            "Iteration:   3% 2/71 [00:01<00:43,  1.60it/s]\u001b[A\n",
            "Iteration:   4% 3/71 [00:01<00:42,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 4/71 [00:02<00:41,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 5/71 [00:03<00:41,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 6/71 [00:03<00:40,  1.61it/s]\u001b[A\n",
            "Iteration:  10% 7/71 [00:04<00:39,  1.60it/s]\u001b[A\n",
            "Iteration:  11% 8/71 [00:04<00:39,  1.60it/s]\u001b[A\n",
            "Iteration:  13% 9/71 [00:05<00:38,  1.59it/s]\u001b[A\n",
            "Iteration:  14% 10/71 [00:06<00:38,  1.60it/s]\u001b[A\n",
            "Iteration:  15% 11/71 [00:06<00:37,  1.60it/s]\u001b[A\n",
            "Iteration:  17% 12/71 [00:07<00:36,  1.60it/s]\u001b[A\n",
            "Iteration:  18% 13/71 [00:08<00:36,  1.60it/s]\u001b[A\n",
            "Iteration:  20% 14/71 [00:08<00:35,  1.61it/s]\u001b[A\n",
            "Iteration:  21% 15/71 [00:09<00:34,  1.61it/s]\u001b[A\n",
            "Iteration:  23% 16/71 [00:09<00:34,  1.61it/s]\u001b[A\n",
            "Iteration:  24% 17/71 [00:10<00:33,  1.61it/s]\u001b[A\n",
            "Iteration:  25% 18/71 [00:11<00:33,  1.60it/s]\u001b[A\n",
            "Iteration:  27% 19/71 [00:11<00:32,  1.60it/s]\u001b[A\n",
            "Iteration:  28% 20/71 [00:12<00:31,  1.60it/s]\u001b[A\n",
            "Iteration:  30% 21/71 [00:13<00:31,  1.60it/s]\u001b[A\n",
            "Iteration:  31% 22/71 [00:13<00:30,  1.61it/s]\u001b[A\n",
            "Iteration:  32% 23/71 [00:14<00:29,  1.60it/s]\u001b[A\n",
            "Iteration:  34% 24/71 [00:14<00:29,  1.61it/s]\u001b[A\n",
            "Iteration:  35% 25/71 [00:15<00:28,  1.61it/s]\u001b[A\n",
            "Iteration:  37% 26/71 [00:16<00:28,  1.59it/s]\u001b[A\n",
            "Iteration:  38% 27/71 [00:16<00:27,  1.60it/s]\u001b[A\n",
            "Iteration:  39% 28/71 [00:17<00:26,  1.60it/s]\u001b[A\n",
            "Iteration:  41% 29/71 [00:18<00:26,  1.60it/s]\u001b[A\n",
            "Iteration:  42% 30/71 [00:18<00:25,  1.60it/s]\u001b[A\n",
            "Iteration:  44% 31/71 [00:19<00:24,  1.60it/s]\u001b[A\n",
            "Iteration:  45% 32/71 [00:19<00:24,  1.60it/s]\u001b[A\n",
            "Iteration:  46% 33/71 [00:20<00:23,  1.60it/s]\u001b[A\n",
            "Iteration:  48% 34/71 [00:21<00:23,  1.60it/s]\u001b[A\n",
            "Iteration:  49% 35/71 [00:21<00:22,  1.61it/s]\u001b[A\n",
            "Iteration:  51% 36/71 [00:22<00:21,  1.61it/s]\u001b[A\n",
            "Iteration:  52% 37/71 [00:23<00:21,  1.60it/s]\u001b[A\n",
            "Iteration:  54% 38/71 [00:23<00:20,  1.61it/s]\u001b[A\n",
            "Iteration:  55% 39/71 [00:24<00:19,  1.61it/s]\u001b[A\n",
            "Iteration:  56% 40/71 [00:24<00:19,  1.61it/s]\u001b[A\n",
            "Iteration:  58% 41/71 [00:25<00:18,  1.60it/s]\u001b[A\n",
            "Iteration:  59% 42/71 [00:26<00:18,  1.60it/s]\u001b[A\n",
            "Iteration:  61% 43/71 [00:26<00:17,  1.61it/s]\u001b[A\n",
            "Iteration:  62% 44/71 [00:27<00:16,  1.60it/s]\u001b[A\n",
            "Iteration:  63% 45/71 [00:28<00:16,  1.61it/s]\u001b[A\n",
            "Iteration:  65% 46/71 [00:28<00:15,  1.61it/s]\u001b[A\n",
            "Iteration:  66% 47/71 [00:29<00:14,  1.61it/s]\u001b[A\n",
            "Iteration:  68% 48/71 [00:29<00:14,  1.61it/s]\u001b[A\n",
            "Iteration:  69% 49/71 [00:30<00:13,  1.61it/s]\u001b[A\n",
            "Iteration:  70% 50/71 [00:31<00:13,  1.60it/s]\u001b[A\n",
            "Iteration:  72% 51/71 [00:31<00:12,  1.61it/s]\u001b[A\n",
            "Iteration:  73% 52/71 [00:32<00:11,  1.61it/s]\u001b[A\n",
            "Iteration:  75% 53/71 [00:33<00:11,  1.60it/s]\u001b[A\n",
            "Iteration:  76% 54/71 [00:33<00:10,  1.60it/s]\u001b[A\n",
            "Iteration:  77% 55/71 [00:34<00:09,  1.60it/s]\u001b[A\n",
            "Iteration:  79% 56/71 [00:34<00:09,  1.61it/s]\u001b[A\n",
            "Iteration:  80% 57/71 [00:35<00:08,  1.60it/s]\u001b[A\n",
            "Iteration:  82% 58/71 [00:36<00:08,  1.60it/s]\u001b[A\n",
            "Iteration:  83% 59/71 [00:36<00:07,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 60/71 [00:37<00:06,  1.58it/s]\u001b[A\n",
            "Iteration:  86% 61/71 [00:38<00:06,  1.59it/s]\u001b[A\n",
            "Iteration:  87% 62/71 [00:38<00:05,  1.59it/s]\u001b[A\n",
            "Iteration:  89% 63/71 [00:39<00:05,  1.60it/s]\u001b[A\n",
            "Iteration:  90% 64/71 [00:39<00:04,  1.60it/s]\u001b[A\n",
            "Iteration:  92% 65/71 [00:40<00:03,  1.60it/s]\u001b[A\n",
            "Iteration:  93% 66/71 [00:41<00:03,  1.61it/s]\u001b[A\n",
            "Iteration:  94% 67/71 [00:41<00:02,  1.61it/s]\u001b[A\n",
            "Iteration:  96% 68/71 [00:42<00:01,  1.61it/s]\u001b[A\n",
            "Iteration:  97% 69/71 [00:43<00:01,  1.61it/s]\u001b[A\n",
            "Iteration:  99% 70/71 [00:43<00:00,  1.61it/s]\u001b[A\n",
            "Iteration: 100% 71/71 [00:43<00:00,  1.62it/s]\n",
            "Epoch: 100% 3/3 [02:11<00:00, 43.88s/it]\n",
            "04/28/2020 16:23:38 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "04/28/2020 16:23:38 - INFO - transformers.trainer -   Saving model checkpoint to output\n",
            "04/28/2020 16:23:38 - INFO - transformers.configuration_utils -   Configuration saved in output/config.json\n",
            "04/28/2020 16:23:39 - INFO - transformers.modeling_utils -   Model weights saved in output/pytorch_model.bin\n",
            "04/28/2020 16:23:40 - INFO - __main__ -   *** Evaluate ***\n",
            "04/28/2020 16:23:40 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "04/28/2020 16:23:40 - INFO - transformers.trainer -     Num examples = 561\n",
            "04/28/2020 16:23:40 - INFO - transformers.trainer -     Batch size = 8\n",
            "Evaluation: 100% 71/71 [00:14<00:00,  5.01it/s]\n",
            "04/28/2020 16:23:54 - INFO - __main__ -   ***** Eval results *****\n",
            "04/28/2020 16:23:54 - INFO - __main__ -     perplexity = 3.264582592048211\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPUjT5xWpPty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KAGGLE_INPUT_PATH = \"../input/\"\n",
        "COLAB_INPUT_PATH = \"/content/drive/My Drive/input/\"\n",
        "INPUT_PATH = COLAB_INPUT_PATH\n",
        "\n",
        "DATA_PATH = INPUT_PATH + \"tweet-sentiment-extraction/\"\n",
        "CHECKPOINT_PATH = INPUT_PATH + \"model_checkpoint/roberta/\"\n",
        "ROBERTA_PATH = INPUT_PATH + 'tf-roberta/'\n",
        "EMOTION_PATH = INPUT_PATH + 'emotion/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEW5LazApUcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd, re, numpy as np\n",
        "def remove_html(text):\n",
        "    text = re.sub(\"&quot;\", '\"', text)\n",
        "    text = re.sub(\"&gt;\", \">\", text)\n",
        "    text = re.sub(\"&lt;\", \"<\", text)\n",
        "    text = re.sub(\"&le;\", \"≤\", text)\n",
        "    text = re.sub(\"&ge;\", \"≥\", text)\n",
        "    text = re.sub(\"&amp;\", \"&\", text)\n",
        "    return text\n",
        "\n",
        "emotion_df=pd.read_csv(EMOTION_PATH + 'text_emotion.csv')\n",
        "emotion_df['content'] = emotion_df['content'].astype(str)\n",
        "emotion_df[\"content\"] = emotion_df[\"content\"].apply(lambda x: \" \".join([i for i in x.split(\" \") if \"@\" not in i]))\n",
        "emotion_df[\"content\"] = emotion_df[\"content\"].apply(remove_html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYFpslP5qFuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "emotion_train, emotion_test = train_test_split(emotion_df, test_size=0.1, random_state=333)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LihB0ceTrt7d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_text(df, filename):\n",
        "    f = open(filename, 'w')\n",
        "    for c in df[\"content\"].values:\n",
        "        f.write(\" \" + c.strip() + \"\\n\\n\")\n",
        "    f.close()\n",
        "to_text(emotion_test, \"../emotion.test.txt\")\n",
        "to_text(emotion_train, \"../emotion.train.txt\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do1tA3RXs21k",
        "colab_type": "code",
        "outputId": "84b20bdb-6b25-4c31-d2af-64b9a19e515f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python examples/run_language_modeling.py \\\n",
        "    --output_dir=output_twitter \\\n",
        "    --model_type=roberta \\\n",
        "    --model_name_or_path=roberta-base \\\n",
        "    --do_train \\\n",
        "    --train_data_file=../emotion.test.txt \\\n",
        "    --do_eval \\\n",
        "    --eval_data_file=../emotion.train.txt \\\n",
        "    --mlm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-28 16:53:57.435816: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "04/28/2020 16:53:59 - INFO - transformers.training_args -   PyTorch: setting up devices\n",
            "04/28/2020 16:53:59 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "04/28/2020 16:53:59 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(output_dir='output_twitter', overwrite_output_dir=False, do_train=True, do_eval=True, do_predict=False, evaluate_during_training=False, per_gpu_train_batch_size=8, per_gpu_eval_batch_size=8, gradient_accumulation_steps=1, learning_rate=5e-05, weight_decay=0.0, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir=None, logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1)\n",
            "04/28/2020 16:54:00 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "04/28/2020 16:54:00 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/28/2020 16:54:01 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.117c81977c5979de8c088352e74ec6e70f5c66096c28b61d3c50101609b39690\n",
            "04/28/2020 16:54:01 - INFO - transformers.configuration_utils -   Model config RobertaConfig {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "04/28/2020 16:54:02 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "04/28/2020 16:54:02 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "04/28/2020 16:54:03 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "04/28/2020 16:54:09 - INFO - transformers.modeling_utils -   Weights of RobertaForMaskedLM not initialized from pretrained model: ['lm_head.decoder.bias']\n",
            "04/28/2020 16:54:09 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ..\n",
            "04/28/2020 16:54:10 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file ../cached_lm_RobertaTokenizer_510_emotion.test.txt [took 0.003 s]\n",
            "04/28/2020 16:54:10 - INFO - transformers.data.datasets.language_modeling -   Creating features from dataset file at ..\n",
            "04/28/2020 16:54:14 - INFO - transformers.data.datasets.language_modeling -   Saving features into cached file ../cached_lm_RobertaTokenizer_510_emotion.train.txt [took 0.023 s]\n",
            "04/28/2020 16:54:18 - INFO - transformers.trainer -   ***** Running training *****\n",
            "04/28/2020 16:54:18 - INFO - transformers.trainer -     Num examples = 147\n",
            "04/28/2020 16:54:18 - INFO - transformers.trainer -     Num Epochs = 3\n",
            "04/28/2020 16:54:18 - INFO - transformers.trainer -     Instantaneous batch size per GPU = 8\n",
            "04/28/2020 16:54:18 - INFO - transformers.trainer -     Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "04/28/2020 16:54:18 - INFO - transformers.trainer -     Gradient Accumulation steps = 1\n",
            "04/28/2020 16:54:18 - INFO - transformers.trainer -     Total optimization steps = 57\n",
            "Epoch:   0% 0/3 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   5% 1/19 [00:00<00:11,  1.51it/s]\u001b[A\n",
            "Iteration:  11% 2/19 [00:01<00:11,  1.54it/s]\u001b[A\n",
            "Iteration:  16% 3/19 [00:01<00:10,  1.55it/s]\u001b[A\n",
            "Iteration:  21% 4/19 [00:02<00:09,  1.57it/s]\u001b[A\n",
            "Iteration:  26% 5/19 [00:03<00:08,  1.58it/s]\u001b[A\n",
            "Iteration:  32% 6/19 [00:03<00:08,  1.58it/s]\u001b[A\n",
            "Iteration:  37% 7/19 [00:04<00:07,  1.59it/s]\u001b[A\n",
            "Iteration:  42% 8/19 [00:05<00:06,  1.59it/s]\u001b[A\n",
            "Iteration:  47% 9/19 [00:05<00:06,  1.59it/s]\u001b[A\n",
            "Iteration:  53% 10/19 [00:06<00:05,  1.59it/s]\u001b[A\n",
            "Iteration:  58% 11/19 [00:06<00:05,  1.60it/s]\u001b[A\n",
            "Iteration:  63% 12/19 [00:07<00:04,  1.60it/s]\u001b[A\n",
            "Iteration:  68% 13/19 [00:08<00:03,  1.60it/s]\u001b[A\n",
            "Iteration:  74% 14/19 [00:08<00:03,  1.60it/s]\u001b[A\n",
            "Iteration:  79% 15/19 [00:09<00:02,  1.60it/s]\u001b[A\n",
            "Iteration:  84% 16/19 [00:10<00:01,  1.60it/s]\u001b[A\n",
            "Iteration:  89% 17/19 [00:10<00:01,  1.60it/s]\u001b[A\n",
            "Iteration:  95% 18/19 [00:11<00:00,  1.60it/s]\u001b[A\n",
            "Iteration: 100% 19/19 [00:11<00:00,  1.64it/s]\n",
            "Epoch:  33% 1/3 [00:11<00:23, 11.59s/it]\n",
            "Iteration:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   5% 1/19 [00:00<00:11,  1.61it/s]\u001b[A\n",
            "Iteration:  11% 2/19 [00:01<00:10,  1.61it/s]\u001b[A\n",
            "Iteration:  16% 3/19 [00:01<00:09,  1.60it/s]\u001b[A\n",
            "Iteration:  21% 4/19 [00:02<00:09,  1.60it/s]\u001b[A\n",
            "Iteration:  26% 5/19 [00:03<00:08,  1.60it/s]\u001b[A\n",
            "Iteration:  32% 6/19 [00:03<00:08,  1.60it/s]\u001b[A\n",
            "Iteration:  37% 7/19 [00:04<00:07,  1.60it/s]\u001b[A\n",
            "Iteration:  42% 8/19 [00:04<00:06,  1.60it/s]\u001b[A\n",
            "Iteration:  47% 9/19 [00:05<00:06,  1.60it/s]\u001b[A\n",
            "Iteration:  53% 10/19 [00:06<00:05,  1.60it/s]\u001b[A\n",
            "Iteration:  58% 11/19 [00:06<00:05,  1.60it/s]\u001b[A\n",
            "Iteration:  63% 12/19 [00:07<00:04,  1.59it/s]\u001b[A\n",
            "Iteration:  68% 13/19 [00:08<00:03,  1.59it/s]\u001b[A\n",
            "Iteration:  74% 14/19 [00:08<00:03,  1.60it/s]\u001b[A\n",
            "Iteration:  79% 15/19 [00:09<00:02,  1.60it/s]\u001b[A\n",
            "Iteration:  84% 16/19 [00:10<00:01,  1.60it/s]\u001b[A\n",
            "Iteration:  89% 17/19 [00:10<00:01,  1.60it/s]\u001b[A\n",
            "Iteration:  95% 18/19 [00:11<00:00,  1.59it/s]\u001b[A\n",
            "Iteration: 100% 19/19 [00:11<00:00,  1.65it/s]\n",
            "Epoch:  67% 2/3 [00:23<00:11, 11.58s/it]\n",
            "Iteration:   0% 0/19 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   5% 1/19 [00:00<00:11,  1.58it/s]\u001b[A\n",
            "Iteration:  11% 2/19 [00:01<00:10,  1.59it/s]\u001b[A\n",
            "Iteration:  16% 3/19 [00:01<00:10,  1.59it/s]\u001b[A\n",
            "Iteration:  21% 4/19 [00:02<00:09,  1.59it/s]\u001b[A\n",
            "Iteration:  26% 5/19 [00:03<00:08,  1.59it/s]\u001b[A\n",
            "Iteration:  32% 6/19 [00:03<00:08,  1.59it/s]\u001b[A\n",
            "Iteration:  37% 7/19 [00:04<00:07,  1.59it/s]\u001b[A\n",
            "Iteration:  42% 8/19 [00:05<00:06,  1.60it/s]\u001b[A\n",
            "Iteration:  47% 9/19 [00:05<00:06,  1.60it/s]\u001b[A\n",
            "Iteration:  53% 10/19 [00:06<00:05,  1.60it/s]\u001b[A\n",
            "Iteration:  58% 11/19 [00:06<00:05,  1.60it/s]\u001b[A\n",
            "Iteration:  63% 12/19 [00:07<00:04,  1.59it/s]\u001b[A\n",
            "Iteration:  68% 13/19 [00:08<00:03,  1.60it/s]\u001b[A\n",
            "Iteration:  74% 14/19 [00:08<00:03,  1.60it/s]\u001b[A\n",
            "Iteration:  79% 15/19 [00:09<00:02,  1.60it/s]\u001b[A\n",
            "Iteration:  84% 16/19 [00:10<00:01,  1.60it/s]\u001b[A\n",
            "Iteration:  89% 17/19 [00:10<00:01,  1.60it/s]\u001b[A\n",
            "Iteration:  95% 18/19 [00:11<00:00,  1.60it/s]\u001b[A\n",
            "Iteration: 100% 19/19 [00:11<00:00,  1.64it/s]\n",
            "Epoch: 100% 3/3 [00:34<00:00, 11.56s/it]\n",
            "04/28/2020 16:54:53 - INFO - transformers.trainer -   \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "04/28/2020 16:54:53 - INFO - transformers.trainer -   Saving model checkpoint to output_twitter\n",
            "04/28/2020 16:54:53 - INFO - transformers.configuration_utils -   Configuration saved in output_twitter/config.json\n",
            "04/28/2020 16:54:54 - INFO - transformers.modeling_utils -   Model weights saved in output_twitter/pytorch_model.bin\n",
            "04/28/2020 16:54:54 - INFO - __main__ -   *** Evaluate ***\n",
            "04/28/2020 16:54:54 - INFO - transformers.trainer -   ***** Running Evaluation *****\n",
            "04/28/2020 16:54:54 - INFO - transformers.trainer -     Num examples = 1312\n",
            "04/28/2020 16:54:54 - INFO - transformers.trainer -     Batch size = 8\n",
            "Evaluation: 100% 164/164 [00:33<00:00,  4.93it/s]\n",
            "04/28/2020 16:55:28 - INFO - __main__ -   ***** Eval results *****\n",
            "04/28/2020 16:55:28 - INFO - __main__ -     perplexity = 17.53027057161708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh1X0SLVtZTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}